<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Photometric Stereo</title>
    <url>/2024/12/09/10-PhotometricStereo/</url>
    <content><![CDATA[<h2 id="一实验目的">一、实验目的</h2>
<p>实验的目的是通过实现光度立体（Photometric
Stereo）算法，利用多张不同光照方向的图像来估计物体表面的法向量和反射率，并根据其来重新渲染指定光照方向下的图像。</p>
<ul>
<li><strong>法向量计算</strong>：根据朗博模型的公式<span
class="math inline">\(I=\rho\overrightarrow{n}\cdot\overrightarrow{l}\)</span>，<span
class="math inline">\(\rho\)</span>是表面的反射率，<span
class="math inline">\(\overrightarrow{n}\)</span>是法向量，<span
class="math inline">\(\overrightarrow{l}\)</span>是光照方向。根据课程介绍的算法，当至少提供三张已知光照方向的图像时，<span
class="math inline">\(\rho\)</span>和<span
class="math inline">\(\overrightarrow{l}\)</span>都可以唯一确定。</li>
<li><strong>阴影高光处理</strong>：阴影和高光打破了线性的朗博模型，一个简单的解决方案是对每个像素的所有观测值排序，丢弃某一百分比的最亮和最暗像素，以去除阴影和高光。</li>
</ul>
<h2 id="二实验原理">二、实验原理</h2>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/12/0dade61b941065d1325153ebd11cd867.png"
                      alt="Lambert" style="zoom: 33%;" 
                ></p>
<p>根据朗博模型（这是一个纯漫反射的模型），对于每一个光线反射点，有个固定的反射率<span
class="math inline">\(\rho_0\)</span>，而反射光线<span
class="math inline">\(L_o\)</span>的强度仅仅与反射率<span
class="math inline">\(\rho_0\)</span>和与入射光线角度<span
class="math inline">\(\theta_i\)</span>（与法向量的夹角）有关。公式如下：
<span class="math display">\[
L_o=L_i\rho_0\cos\theta_i=L_i\rho_0\boldsymbol{n}\cdot\boldsymbol{l}
\]</span>
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/12/a0fd7c0140bfcafeb008f056358914a7.png"
                      alt="image-20241207132846933" style="zoom: 33%;" 
                ></p>
<p>当我们拥有不同光照方向的图像时，就可以利用朗博模型对每个像素点进行单独的分析了。当我们有三个光照角度的图像时，可以以矩阵形式获得朗博模型下的等式：
<span class="math display">\[
\begin{pmatrix}
I_1 \\
I_2 \\
I_3
\end{pmatrix}=
\begin{pmatrix}
\boldsymbol{l}_1^T \\
\boldsymbol{l}_2^T \\
\boldsymbol{l}_3^T
\end{pmatrix}\mathrm{\rho}\boldsymbol{n}
\]</span> 由于反射光强<span
class="math inline">\(I_i\)</span>和入射光角度<span
class="math inline">\(\boldsymbol{l}_i\)</span>都是已知的，所以通过三个光照角度的数据，就可以通过简单的矩阵求逆获得反射率<span
class="math inline">\(\mathrm{\rho}\)</span>和法向量<span
class="math inline">\(\boldsymbol{n}\)</span>。</p>
<p>知道反射率和法向量的基础上，再对指定光照角度的图像渲染就很容易了，直接根据朗博模型求每个像素的光照即可。</p>
<h2 id="三实验内容">三、实验内容</h2>
<p>在这次实验给出的代码框架里，需要自己实现的基本上只有<code>myPMS.m</code>文件。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/12/53166fad82ee50e249ecc6a5f518cde9.png"
                      alt="image-20241207134101820" style="zoom:50%;" 
                ></p>
<p>首先，对原本给出的函数声明进行了改动，因为最后要求输出的三幅图像，但是给出的框架里面又只输出了法向量，所以把反射率图像和重渲染图像也加到函数返回里了。然后函数输入新增了一个<code>shadow_removal_percentage</code>，它表示我在处理阴影高光是要舍弃的最亮最暗的像素值的百分比（比如它的值如果是20，那我要舍弃最暗的20%和最亮的20%）。</p>
<p>在数据预处理阶段，用<code>N</code>、<code>albedo</code>、<code>re_rendered_img</code>分别表示法向量图、反射率图和重渲染图。它们都是三通道的。<code>I</code>用来暂存三个通道的光照强度值。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/12/1d0863310ac9b5c8a1ee62cfa2fa5735.png"
                      alt="image-20241207134531010" style="zoom:50%;" 
                ></p>
<p>通过简单的循环遍历，先是对原图像经过了<code>mask</code>，通过遮罩把主体提取出来，防止背景的影响。然后在RGB的每个通道分别除以给出的<code>light_intensity</code>，获取每个通道真实的光照强度<code>I</code>。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/12/12820516d1dd2c93dc4760f293d792fc.png"
                      alt="image-20241207134953399" style="zoom:50%;" 
                ></p>
<p>然后再遍历每一个像素点，对所有图像的光照强度进行排序，舍弃最亮和最暗的给定百分比的部分，存放到<code>I_col_filtered</code>里面。根据公式<span
class="math inline">\(I=\rho\overrightarrow{n}\cdot\overrightarrow{l}\)</span>，可以先反推<span
class="math inline">\(\rho\)</span>，简单的通过光照方向<code>s_filtered</code>和光照强度<code>I_col_filtered</code>作最小二乘法，即可获得每个像素点的反射率和法向量的积<code>A</code>。对<code>A</code>求norm即可获得反射率的值，然后对<code>A</code>除以这个反射率即可获得单位法向量。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/12/20fdbf89aed0546975b7f8d49893a762.png"
                      alt="image-20241207140409970" style="zoom:50%;" 
                ></p>
<p>有以上的信息，重渲染就很简单了。直接遍历对每个像素通过朗博模型的公式套上去就可以了。但是由于作业要求里好像没有要求入射光的RGB分量，所以就默认都为1了。</p>
<p>至此，光度立体法的主体函数就完成了，以下是对Baseline进行的小修改：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/12/0ace3623602bd317629dd0b1f47f7056.png"
                      alt="image-20241207140539892" style="zoom:50%;" 
                ></p>
<p>首先函数调用这块，按照我改之后的输入输出来的，这里为处理阴影高光舍弃的百分比为20，即丢弃最暗的20%和最亮的20%。为了最后显示的重渲染图像更美观，这里还将它进行了归一化。</p>
<p>光度立体法的具体代码如下(可展开折叠)：</p>
<p>（在我的github仓库<a class="link" 
 href="https://github.com/FelixChristian011226/ComputerVision/tree/main/Homework%201" >ComputerVision/Homework
1<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>也可以找到）</p>
<details class="orange" data-header-exclude><summary><i class="fa-solid fa-chevron-right"></i>Folding 点击查看更多 </summary>
              <div class='content'>
              <div class="code-container" data-rel="Matlab"><figure class="iseeu highlight matlab"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[N, albedo, re_rendered_img]</span> = <span class="title">L2_PMS</span><span class="params">(data, m, shadow_removal_percentage)</span></span></span><br><span class="line"></span><br><span class="line">    num_images = <span class="built_in">size</span>(data.s, <span class="number">1</span>);</span><br><span class="line">    [height, width, ~] = <span class="built_in">size</span>(data.imgs&#123;<span class="number">1</span>&#125;);</span><br><span class="line">    </span><br><span class="line">    N = <span class="built_in">zeros</span>(height, width, <span class="number">3</span>);</span><br><span class="line">    albedo = <span class="built_in">zeros</span>(height, width, <span class="number">3</span>);</span><br><span class="line">    re_rendered_img = <span class="built_in">zeros</span>(height, width, <span class="number">3</span>);</span><br><span class="line">    I = <span class="built_in">zeros</span>(num_images, <span class="built_in">length</span>(m), <span class="number">3</span>);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">% Extract pixel intensities for each image</span></span><br><span class="line">    <span class="keyword">for</span> c = <span class="number">1</span>:<span class="number">3</span></span><br><span class="line">        <span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:num_images</span><br><span class="line">            img = double(data.imgs&#123;<span class="built_in">i</span>&#125;);</span><br><span class="line">            img = img(m);</span><br><span class="line">            img = img / data.L(<span class="built_in">i</span>, c);</span><br><span class="line">            I(<span class="built_in">i</span>, :, c) = img;</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> c = <span class="number">1</span>:<span class="number">3</span></span><br><span class="line">    <span class="comment">% Remove shadows and highlights, compute normals and albedo</span></span><br><span class="line">        <span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:<span class="built_in">length</span>(m)</span><br><span class="line">            I_col = I(:, <span class="built_in">i</span>, c);</span><br><span class="line">            </span><br><span class="line">            <span class="comment">% Sort intensities and remove shadows/highlights</span></span><br><span class="line">            [sorted_I, idx] = <span class="built_in">sort</span>(I_col);</span><br><span class="line">            num_to_remove = <span class="built_in">round</span>(<span class="built_in">length</span>(sorted_I) * shadow_removal_percentage / <span class="number">100</span>);</span><br><span class="line">            valid_idx = idx(num_to_remove+<span class="number">1</span>:<span class="keyword">end</span>-num_to_remove);</span><br><span class="line">            </span><br><span class="line">            <span class="comment">% Filter light source directions and intensities</span></span><br><span class="line">            s_filtered = data.s(valid_idx, :);</span><br><span class="line">            I_col_filtered = sorted_I(num_to_remove+<span class="number">1</span>:<span class="keyword">end</span>-num_to_remove);</span><br><span class="line">            </span><br><span class="line">            <span class="comment">% Solve for normal and albedo</span></span><br><span class="line">            A = s_filtered \ I_col_filtered;</span><br><span class="line">            albedo_val = norm(A);</span><br><span class="line"></span><br><span class="line">            norm_A = A / albedo_val;</span><br><span class="line">            [row, col] = <span class="built_in">ind2sub</span>([height, width], m(<span class="built_in">i</span>));</span><br><span class="line">            N(row, col, :) = norm_A&#x27;;</span><br><span class="line">            albedo(row, col, c) = albedo_val;</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    <span class="comment">% Re-render the image using recovered normals and albedo</span></span><br><span class="line">    viewing_direction = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>];</span><br><span class="line">    <span class="keyword">for</span> c = <span class="number">1</span>:<span class="number">3</span></span><br><span class="line">        <span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:height</span><br><span class="line">            <span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span>:width</span><br><span class="line">                normal = <span class="built_in">squeeze</span>(N(<span class="built_in">i</span>, <span class="built_in">j</span>, :));</span><br><span class="line">                <span class="keyword">if</span> norm(normal) &gt; <span class="number">0</span></span><br><span class="line">                    re_rendered_img(<span class="built_in">i</span>, <span class="built_in">j</span>, c) = <span class="built_in">max</span>(<span class="number">0</span>, <span class="built_in">dot</span>(normal, viewing_direction)) * albedo(<span class="built_in">i</span>, <span class="built_in">j</span>, c);</span><br><span class="line">                <span class="keyword">end</span></span><br><span class="line">            <span class="keyword">end</span></span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    </span><br></pre></td></tr></table></figure></div>
              </div>
            </details>
<h2 id="四实验结果">四、实验结果</h2>
<table>
<colgroup>
<col style="width: 3%" />
<col style="width: 32%" />
<col style="width: 32%" />
<col style="width: 32%" />
</colgroup>
<thead>
<tr>
<th></th>
<th>Normal Map</th>
<th>Albedo Map</th>
<th>Re-rendered Picture</th>
</tr>
</thead>
<tbody>
<tr>
<td>bear</td>
<td><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/12/5ae7f5df5e480e32313eeadfc218a1ec.png"
                      alt="bearPNG_Normal"  
                ></td>
<td><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/12/11126f4fba79a877396cbb649b85eae2.png"
                      alt="bearPNG_Albedo"  
                ></td>
<td><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/12/b1e038833411195869dccfc94e9311c9.png"
                      alt="bearPNG_ReRendered"  
                ></td>
</tr>
<tr>
<td>buddha</td>
<td><img 

                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/12/77db0ed558a8954d54ed24bd548b61ec.png"
                     
alt="buddhaPNG_Normal" 
                ></td>
<td><img 

                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/12/624f4c0c73b765a0b7725128394ead80.png"
                     
alt="buddhaPNG_Albedo" 
                ></td>
<td><img 

                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/12/551786fccd59bfbe22ee559df3473d6d.png"
                     
alt="buddhaPNG_ReRendered" 
                ></td>
</tr>
<tr>
<td>cat</td>
<td><img 

                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/12/1ca2781d392d4258eee056952fa7ac6e.png"
                     
alt="catPNG_Normal" 
                ></td>
<td><img 

                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/12/63a725360b18d28c825f364ab06f1c4f.png"
                     
alt="catPNG_Albedo" 
                ></td>
<td><img 

                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/12/e897663e32bde8e338e11397699d2ed1.png"
                     
alt="catPNG_ReRendered" 
                ></td>
</tr>
<tr>
<td>pot</td>
<td><img 

                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/12/198d54d72124b26b50beea61be1a33a2.png"
                     
alt="potPNG_Normal" 
                ></td>
<td><img 

                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/12/a6ed46b80000be1e8402a751e14fdbe4.png"
                     
alt="potPNG_Albedo" 
                ></td>
<td><img 

                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/12/4bf4ac3fb69d12f957b4f42600ebb596.png"
                     
alt="potPNG_ReRendered" 
                ></td>
</tr>
</tbody>
</table>
]]></content>
      <categories>
        <category>研究生课程</category>
        <category>计算机视觉</category>
      </categories>
      <tags>
        <tag>Photometric Stereo</tag>
      </tags>
  </entry>
  <entry>
    <title>Mujoco - 高场hfield相关</title>
    <url>/2024/09/26/1-Hfield/</url>
    <content><![CDATA[<blockquote>
<p>The <strong>hfield</strong> type defines a height field geom. The
geom must reference the desired height field asset with the hfield
attribute below. The position and orientation of the geom set the
position and orientation of the height field. The size of the geom is
ignored, and the size parameters of the height field asset are used
instead. See the description of the <a class="link" 
 href="https://mujoco.readthedocs.io/en/stable/XMLreference.html#asset-hfield" >hfield<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>
element. Similar to planes, height field geoms can only be attached to
the world body or to static children of the world.</p>
</blockquote>
<ol type="1">
<li><p>可从PNG的灰度图像加载高场数据。每个像素即为一个高度，黑低白高。</p></li>
<li><p>可从bin文件读入，格式如下：</p>
<div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">(int32)   nrow</span><br><span class="line">(int32)   ncol</span><br><span class="line">(float32) data[nrow*ncol]</span><br></pre></td></tr></table></figure></div></li>
<li><p>高度数据可以在编译时保持未定义。</p></li>
</ol>
<ul>
<li>编译器会自动把高度数据归一化到[0,1]</li>
<li>高场的位置和方向由geom确定，空间范围由hfield的size字段决定。（与mesh相同）</li>
<li>高场视为三棱柱的并集，碰撞时首先确认可能碰撞的棱柱网格，然后通过凸面碰撞器计算。高场和geom的碰撞上限限制为50，超过的则被舍弃。</li>
</ul>
<h2 id="参数">参数</h2>
<ul>
<li><strong>name</strong>:
名称，用于引用。如果忽略name，可用不带路径和后缀名的文件名代替引用。</li>
<li><strong>content_type</strong>:
目前支持<code>image/png</code>和<code>image/vnd.mujoco.hfield</code>。</li>
<li><strong>file</strong>:
文件名，若后缀为<code>.png</code>（不区分大小写），则按图像读入；否则以二进制文件读入。</li>
<li><strong>nrow</strong>, <strong>ncol</strong>: 行数和列数。默认值 0
表示将从文件加载数据。</li>
<li><strong>elevation</strong>: 高场，自动归一，默认值0。</li>
<li><strong>size</strong>:
(radius_x、radius_y、elevation_z、base_z)，分别是x、y方向的半径，最大高度，和基础厚度。</li>
</ul>
<h2 id="使用样例">使用样例</h2>
<div class="code-container" data-rel="Xml"><figure class="iseeu highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">mujoco</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">asset</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">hfield</span> <span class="attr">file</span>=<span class="string">&quot;./data/height_field.bin&quot;</span> <span class="attr">name</span>=<span class="string">&quot;customTerrain&quot;</span> <span class="attr">ncol</span>=<span class="string">&quot;100&quot;</span> <span class="attr">nrow</span>=<span class="string">&quot;100&quot;</span> <span class="attr">size</span>=<span class="string">&quot;50 50 1 0.1&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">asset</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">worldbody</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">geom</span> <span class="attr">hfield</span>=<span class="string">&quot;customTerrain&quot;</span> <span class="attr">pos</span>=<span class="string">&quot;0 0 0&quot;</span> <span class="attr">type</span>=<span class="string">&quot;hfield&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">worldbody</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">mujoco</span>&gt;</span></span><br></pre></td></tr></table></figure></div>
]]></content>
      <categories>
        <category>机器仿真</category>
      </categories>
      <tags>
        <tag>mujoco</tag>
        <tag>hfield</tag>
      </tags>
  </entry>
  <entry>
    <title>将现有场景生成网格导入mujoco的简要pipeline</title>
    <url>/2024/12/10/11-MeshToMujoco/</url>
    <content><![CDATA[<h2 id="一数据预处理">一、数据预处理</h2>
<p>nerfstudio里整合了COLMAP工具，可通过它的指令直接对现有的数据进行预处理，转化为nerfstudio需要的格式。支持的数据类型如下表所示：</p>
<table>
<colgroup>
<col style="width: 29%" />
<col style="width: 29%" />
<col style="width: 29%" />
<col style="width: 11%" />
</colgroup>
<thead>
<tr>
<th>Data</th>
<th>Capture Device</th>
<th>Requirements</th>
<th><code>ns-process-data</code> Speed</th>
</tr>
</thead>
<tbody>
<tr>
<td>📷 <a class="link" 
 href="https://docs.nerf.studio/quickstart/custom_dataset.html#images-or-video" >Images<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></td>
<td>Any</td>
<td><a class="link"   href="https://colmap.github.io/install.html" >COLMAP<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></td>
<td>🐢</td>
</tr>
<tr>
<td>📹 <a class="link" 
 href="https://docs.nerf.studio/quickstart/custom_dataset.html#images-or-video" >Video<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></td>
<td>Any</td>
<td><a class="link"   href="https://colmap.github.io/install.html" >COLMAP<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></td>
<td>🐢</td>
</tr>
<tr>
<td>🌎 <a class="link" 
 href="https://docs.nerf.studio/quickstart/custom_dataset.html#data-equirectangular" >360
Data<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></td>
<td>Any</td>
<td><a class="link"   href="https://colmap.github.io/install.html" >COLMAP<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></td>
<td>🐢</td>
</tr>
<tr>
<td>📱 <a class="link" 
 href="https://docs.nerf.studio/quickstart/custom_dataset.html#polycam-capture" >Polycam<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></td>
<td>IOS with LiDAR</td>
<td><a class="link"   href="https://poly.cam/" >Polycam App<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></td>
<td>🐇</td>
</tr>
<tr>
<td>📱 <a class="link" 
 href="https://docs.nerf.studio/quickstart/custom_dataset.html#kiri-engine-capture" >KIRI
Engine<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></td>
<td>IOS or Android</td>
<td><a class="link"   href="https://www.kiriengine.com/" >KIRI Engine App<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></td>
<td>🐇</td>
</tr>
<tr>
<td>📱 <a class="link" 
 href="https://docs.nerf.studio/quickstart/custom_dataset.html#record3d-capture" >Record3D<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></td>
<td>IOS with LiDAR</td>
<td><a class="link"   href="https://record3d.app/" >Record3D app<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></td>
<td>🐇</td>
</tr>
<tr>
<td>📱 <a class="link" 
 href="https://docs.nerf.studio/quickstart/custom_dataset.html#spectacularai" >Spectacular
AI<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></td>
<td>IOS, OAK, <a class="link" 
 href="https://www.spectacularai.com/mapping#supported-devices" >others<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></td>
<td><a class="link" 
 href="https://apps.apple.com/us/app/spectacular-rec/id6473188128" >App<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>
/ <a
href="https://www.spectacularai.com/mapping"><code>sai-cli</code></a></td>
<td>🐇</td>
</tr>
<tr>
<td>🖥 <a class="link" 
 href="https://docs.nerf.studio/quickstart/custom_dataset.html#metashape" >Metashape<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></td>
<td>Any</td>
<td><a class="link"   href="https://www.agisoft.com/" >Metashape<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></td>
<td>🐇</td>
</tr>
<tr>
<td>🖥 <a class="link" 
 href="https://docs.nerf.studio/quickstart/custom_dataset.html#realitycapture" >RealityCapture<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></td>
<td>Any</td>
<td><a class="link" 
 href="https://www.capturingreality.com/realitycapture" >RealityCapture<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></td>
<td>🐇</td>
</tr>
<tr>
<td>🖥 <a class="link" 
 href="https://docs.nerf.studio/quickstart/custom_dataset.html#odm" >ODM<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></td>
<td>Any</td>
<td><a class="link"   href="https://github.com/OpenDroneMap/ODM" >ODM<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></td>
<td>🐇</td>
</tr>
<tr>
<td>👓 <a class="link" 
 href="https://docs.nerf.studio/quickstart/custom_dataset.html#aria" >Aria<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></td>
<td>Aria glasses</td>
<td><a class="link"   href="https://projectaria.com/" >Project Aria<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></td>
<td>🐇</td>
</tr>
<tr>
<td>🛠 <a class="link" 
 href="https://docs.nerf.studio/quickstart/data_conventions.html" >Custom<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></td>
<td>Any</td>
<td>Camera Poses</td>
<td>🐇</td>
</tr>
</tbody>
</table>
<p>一个简单的处理的代码示例如下： <div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">ns-process-data &#123;video,images,polycam,record3d&#125; --data &#123;DATA_PATH&#125; --output-dir &#123;PROCESSED_DATA_DIR&#125;</span><br></pre></td></tr></table></figure></div></p>
<p>完整的参数集可参考<a class="link" 
 href="https://docs.nerf.studio/reference/cli/ns_process_data.html" >此处<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>。在<a class="link" 
 href="https://docs.nerf.studio/quickstart/custom_dataset.html" >教程网站<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>也有更多其他类型数据的捕获教程。</p>
<details class="orange" data-header-exclude><summary><i class="fa-solid fa-chevron-right"></i>踩坑 点击展开 </summary>
              <div class='content'>
              <p class='h3' id="一colmap-feature_extractor报错">(一)、colmapfeature_extractor报错：</h3><p><strong>报错内容</strong>：</p><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/12/a782ca87807e6116d05be3d9526f7ff9.png"
                      alt="image-20241210152913623" style="zoom: 67%;" 
                ></p><p><strong>解决方案</strong>：</p><ol type="1"><li><p>确认<code>xcb</code>插件是否安装：</p><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">sudo</span> apt update</span><br><span class="line"><span class="built_in">sudo</span> apt install libxcb-*</span><br></pre></td></tr></table></figure></div></li><li><p>设置环境变量：</p><div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> QT_QPA_PLATFORM_PLUGIN_PATH=/home/felix/.local/lib/python3.10/site-packages/cv2/qt/plugins</span><br><span class="line"><span class="built_in">export</span> QT_QPA_PLATFORM=offscreen</span><br></pre></td></tr></table></figure></div><p>（路径记得改成自己的路径）</p></li></ol><p>这样应该就可以了，至少博主执行这两串代码后能成功运行。（虽然这个处理过程确实很慢）</p>
              </div>
            </details>
<h2 id="二模型训练">二、模型训练</h2>
<p>可直接对处理后的数据用nerfacto方法进行训练：</p>
<div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">ns-train nerfacto --data &#123;PROCESSED_DATA_DIR&#125;</span><br></pre></td></tr></table></figure></div>
<p>还有一些比较常用的参数：</p>
<div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">--<span class="built_in">help</span> <span class="comment">#显示帮助菜单</span></span><br><span class="line">--output-dir <span class="comment">#输出文件夹</span></span><br><span class="line">--steps-per-save <span class="comment">#多少step保存一次，默认为2000</span></span><br><span class="line">--max-num-iterations <span class="comment">#最大iteration，默认为30000</span></span><br><span class="line">--mixed-precision <span class="comment">#是否启用混合精度，默认为true</span></span><br><span class="line">--load-config <span class="comment">#从配置文件读取，可从之前的训练中继续</span></span><br></pre></td></tr></table></figure></div>
<p>训练结果默认会保存在outputs文件夹中。其中包含一个<code>config.yml</code>文件，记录了训练的配置信息。可通过读取它来进行接续的训练或者可视化：</p>
<div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">ns-viewer --load-config &#123;outputs/.../config.yml&#125;</span><br></pre></td></tr></table></figure></div>
<h2 id="三模型导出">三、模型导出</h2>
<p>在nerfstudio里支持三种类型的模型导出：<code>3d gaussion</code>、<code>point cloud</code>和<code>mesh</code>。这里只提及<code>mesh</code>的导出。</p>
<p>最简单的导出指令为：</p>
<div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">ns-export poisson --load-config CONFIG.yml --output-dir OUTPUT_DIR</span><br></pre></td></tr></table></figure></div>
<p>而在可视化界面中，可以自动生成导出指令：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/12/291b1e3654b600855e834fc4b8800818.png"
                      alt="image-20241210172317358" style="zoom:80%;" 
                ></p>
<p>其中可以对参数进行调整，比如<code>Use Crop</code>可以自动裁剪保留中心物体，<code># Faces</code>可以控制产生的面数的上限，<code># Points</code>控制点数的上限。最后生成的指令为以下结构：</p>
<div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">ns-export poisson --load-config &#123;outputs/.../config.yml&#125; --output-dir &#123;OUTPUT_DIR&#125; --target-num-faces 50000 --num-pixels-per-side 2048 --num-points 1000000 --remove-outliers True --normal-method open3d --obb_center 0.0000000000 0.0000000000 0.0000000000 --obb_rotation 0.0000000000 0.0000000000 0.0000000000 --obb_scale 1.0000000000 1.0000000000 1.0000000000</span><br></pre></td></tr></table></figure></div>
<p>其中<code>possion</code>
表示使用泊松重建方法，nerfstudio还支持其他的重建方法，如<code>marching-cubes</code>和<code>tsdf</code>等，但效果都不尽如人意（可参考我之前的文章<a class="link" 
 href="https://felixchristian.top/2024/11/12/8-NeRF_Studio/" >NeRF
Studio简要教程 | FelixChristian's Blog<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>，里面有重建结果）。</p>
<h2 id="四凸分解">四、凸分解</h2>
<p>在我之前的文章<a class="link" 
 href="https://felixchristian.top/2024/09/29/3-CoACD_notes/" >Mujoco -
CoACD简略教程 | FelixChristian's
Blog<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>中有介绍到CoACD工具的安装和使用。</p>
<p>当编译完CoACD之后，可直接执行凸分解指令： <div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">./main -i PATH_OF_YOUR_MESH -o PATH_OF_OUTPUT</span><br></pre></td></tr></table></figure></div></p>
<p>需要注意的是，两个<code>PATH</code>都是需要以具体的<code>.obj</code>文件结尾的。最终导出的单个<code>.obj</code>文件是以多个组件结合的。如果直接导入mujoco，它还是会对整个文件再次进行替换。</p>
<p>为了在mujoco中实现正常的碰撞。必须把单个<code>.obj</code>文件导出为多个<code>.stl</code>文件，再在mujoco的<code>.xml</code>文件中进行引用。</p>
<p>这里借助了Blender的<a class="link" 
 href="https://github.com/mrtripie/Blender-Super-Batch-Export" >Super-Batch-Export<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>插件：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/12/1a9a797389874618f4db594e7c9dcdd9.png"
                      alt="image-20241210190612219" style="zoom:80%;" 
                ></p>
<p>需要注意的是，根据mujoco的官方文档介绍：</p>
<blockquote>
<p>MuJoCo can load triangulated meshes from OBJ files and binary STL.
Software such as <a class="link"   href="https://www.meshlab.net/" >MeshLab<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a> can be
used to convert from other formats. While any collection of triangles
can be loaded and visualized as a mesh, the collision detector works
with the convex hull. There are compile-time options for scaling the
mesh, as well as fitting a primitive geometric shape to it. The mesh can
also be used to automatically infer inertial properties – by treating it
as a union of triangular pyramids and combining their masses and
inertias. Note that meshes have no color, instead the mesh is colored
using the material properties of the referencing geom. In contrast, all
spatial properties are determined by the mesh data. MuJoCo supports both
OBJ and a custom binary file format for normals and texture coordinates.
Meshes can also be embedded directly in the XML.</p>
</blockquote>
<p>mujoco可以从<code>.obj</code>文件和<code>.stl</code>文件加载三角网格，所以在用Blender进行批量导出时，需要选择这两种格式。</p>
<h2 id="五导入mujoco">五、导入mujoco</h2>
<p>以<code>.obj</code>文件为例（<code>.stl</code>文件同理）：</p>
<p>在上一步中导出的文件为<code>convex_0.obj</code>、<code>convex_1.obj</code>、...
、<code>convex_n.obj</code>的形式。在我的<a class="link" 
 href="https://github.com/FelixChristian011226/Mujoco-RL" >FelixChristian011226/Mujoco-RL<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>仓库中，编写了一个简单的脚本<a class="link" 
 href="https://github.com/FelixChristian011226/Mujoco-RL/blob/main/terrain/gen_component/gen_convex.py" >gen_convex.py<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>，用于生成<code>.xml</code>文件中相应的引用。使用过程如下：</p>
<ol type="1">
<li><p>首先，将生成的多个<code>.obj</code>文件，存放在<code>./terrain/mesh/&#123;your_mesh_name&#125;/</code>文件夹中。</p></li>
<li><p>然后进入<code>./terrain</code>目录，使用如下指令：</p>
<div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">python3 ./gen_component/gen_convex.py &#123;model_folder&#125; &#123;indent_level&#125; &#123;total_count&#125;</span><br></pre></td></tr></table></figure></div>
<p>其中<code>model_folder</code>对应<code>your_mesh_name</code>，即<code>./terrain/mesh/</code>目录下存放<code>.obj</code>文件的文件夹名称。</p>
<p><code>indent_level</code>是缩进量，一般为2，仅仅为了<code>.xml</code>文件中美观而设置。</p>
<p><code>total_count</code>是总的<code>.obj</code>文件数量，即最后一个文件的下标加一。</p></li>
<li><p>最后将<code>.terrain/component/convex.xml</code>文件中的前半部分内容，复制到对应的<code>.xml</code>的<code>&lt;asset&gt;</code>部分，后半部分内容复制到<code>&lt;worldbody&gt;</code>部分。然后给其设置纹理，如：</p>
<div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">&lt;material name=&quot;mat_&#123;name&#125;&quot; rgba=&quot;0.8 0.8 0.8 1&quot;/&gt;</span><br></pre></td></tr></table></figure></div>
<p>其中<code>name</code>应该和文件夹名对应。</p></li>
<li><p>最后再运行对应的<code>.xml</code>文件即可。</p></li>
</ol>
]]></content>
      <categories>
        <category>机器仿真</category>
      </categories>
      <tags>
        <tag>mujoco</tag>
        <tag>mesh</tag>
        <tag>NeRF</tag>
        <tag>CoACD</tag>
      </tags>
  </entry>
  <entry>
    <title>Mujoco - 碰撞凸几何体要求</title>
    <url>/2024/09/29/2-Mujoco_notes/</url>
    <content><![CDATA[<h2 id="collision">Collision</h2>
<p><a class="link" 
 href="https://mujoco.readthedocs.io/en/stable/computation/index.html#collision" >Computation
- MuJoCo Documentation<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>
<blockquote>
<p>We have chosen to limit collision detection to <em>convex</em> geoms.
All primitive types are convex. Height fields are not convex but
internally they are treated as unions of triangular prisms (using custom
collision pruning beyond the filters described above). Meshes specified
by the user can be non-convex, and are rendered as such. For collision
purposes however they are replaced with their convex hulls.</p>
</blockquote>
<ul>
<li>碰撞检测限制在<em>凸</em>几何体</li>
<li>所有原始类型都是凸的</li>
<li>高度字段不是凸的，但在内部它们被视为三棱柱的并集</li>
<li>网格可以是非凸的，并且如此渲染。然而，出于碰撞目的，它们被替换为凸包</li>
</ul>
<blockquote>
<p>In order to model a non-convex object other than a height field, the
user must decompose it into a union of convex geoms (which can be
primitive shapes or meshes) and attach them to the same body. Open tools
like the <a class="link"   href="https://github.com/SarahWeiii/CoACD" >CoACD library<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>
can be used outside MuJoCo to automate this process. Finally, all
built-in collision functions can be replaced with custom callbacks. This
can be used to incorporate a general-purpose “triangle soup” collision
detector for example. However we do not recommend such an approach.
Pre-processing the geometry and representing it as a union of convex
geoms takes some work, but it pays off at runtime and yields both faster
and more stable simulation.</p>
<p>为了对高度场以外的非凸对象进行建模，用户必须将其分解为凸几何体（可以是原始形状或网格）的并集并将它们附加到同一实体。可以在
MuJoCo 外部使用<a class="link"   href="https://github.com/SarahWeiii/CoACD" >CoACD
库<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>等开放工具来自动化此过程。最后，所有内置碰撞函数都可以替换为自定义回调。例如，这可用于合并通用“三角汤”碰撞检测器。但是我们不推荐这种方法。预处理几何体并将其表示为凸几何体的联合需要一些工作，但它在运行时得到回报，并产生更快、更稳定的模拟。</p>
</blockquote>
]]></content>
      <categories>
        <category>机器仿真</category>
      </categories>
      <tags>
        <tag>mujoco</tag>
        <tag>collision</tag>
        <tag>convex</tag>
      </tags>
  </entry>
  <entry>
    <title>GauU-Scene V2 论文解读</title>
    <url>/2024/11/05/5-GauU_Scene_V2/</url>
    <content><![CDATA[<p>论文：</p>
<blockquote>
<p>GauU-Scene V2: Assessing the Reliability of Image-Based Metrics with
Expansive Lidar Image Dataset Using 3DGS and NeRF</p>
</blockquote>
<h2 id="数据集">数据集</h2>
<figure>
<img 

                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/db7529b3c3e21b5e4cb4c591de2c70cb.png"
                     
alt="dataset" 
                >
<figcaption aria-hidden="true">dataset</figcaption>
</figure>
<table>
<colgroup>
<col style="width: 15%" />
<col style="width: 41%" />
<col style="width: 43%" />
</colgroup>
<thead>
<tr>
<th>方法</th>
<th>做法</th>
<th>缺点</th>
</tr>
</thead>
<tbody>
<tr>
<td>bungeenerf</td>
<td>卫星捕获图像</td>
<td>时间差异、无地面实况</td>
</tr>
<tr>
<td>KITTI</td>
<td>汽车雷达捕获点云数据</td>
<td>屋顶、高层建筑捕获存在不足</td>
</tr>
<tr>
<td>blockNeRF</td>
<td>Style Transformation解决时间差异</td>
<td>不提供公开可用的点云数据集</td>
</tr>
<tr>
<td>UrbanBIS</td>
<td>多视图相机捕获点云</td>
<td>未用高精度激光雷达</td>
</tr>
<tr>
<td>Urbanscene3D</td>
<td>无人机配合激光雷达</td>
<td>坐标差异，雷达点云和图像关系不明确</td>
</tr>
</tbody>
</table>
<p><strong>优势：</strong></p>
<ul>
<li>利用Zenmuse L1来获取地面真实几何，而大多数数据集（ blocknerf ）（
megaNeRF ）（ UrbanBIS
）依赖单目或多视图相机进行数据采集，这更适合新视图合成而不是场景重建。</li>
<li>提供了城市规模的信息，包括高层建筑、湖泊、山脉和屋顶，而其他数据集很少提供。</li>
<li>double-return技术，去除移动物体，确保更稳定的光照效果。</li>
<li>去除飞行路线中连续图像之间的冗余信息，图像更少，但信息量仍然具有可比性。</li>
</ul>
<h2 id="评估指标">评估指标</h2>
<h3 id="psnr">PSNR</h3>
<p>PSNR(Peak signal-to-noise ratio 峰值信噪比)
用于表示信号的最大可能功率与影响其表示的保真度的破坏噪声的功率之间的比率，通常使用分贝标度表示为对数量。</p>
<figure>
<img 

                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/d6245af1d4a65be444fc779e62074667.png"
                     
alt="MSE" 
                >
<figcaption aria-hidden="true">MSE</figcaption>
</figure>
<figure>
<img 

                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/d0eb05ab65303b4ae37eabbfc43212b7.png"
                     
alt="PSNR" 
                >
<figcaption aria-hidden="true">PSNR</figcaption>
</figure>
<h3 id="ssim">SSIM</h3>
<p>SSIM全称为Structural
Similarity，即结构相似性。算法会提取以下三个特征。</p>
<ul>
<li><strong>亮度</strong></li>
<li><strong>对比度</strong></li>
<li><strong>结构</strong></li>
</ul>
<p>亮度的估计与平均灰度有关：<img 

                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/cfd417d0ced2bd8f64a88b7fa799f8c5.png"
                     
alt="均值" 
                ></p>
<p>对比度的估计则用标准差衡量：<img 

                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/ff69fd63d0e036d0866f420595dc2668.png"
                     
alt="标准差" 
                ></p>
<p>结构比较是通过使用一个合并公式来完成。</p>
<p>三个对比函数分别如下：</p>
<table>
<colgroup>
<col style="width: 14%" />
<col style="width: 85%" />
</colgroup>
<thead>
<tr>
<th></th>
<th>公式</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>亮度</strong></td>
<td><img 

                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/9615320354732921de0c9a7268823ca3.png"
                     
alt="亮度" 
                ></td>
</tr>
<tr>
<td><strong>对比度</strong></td>
<td><img 

                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/f82417d40a829e317089fc1240823192.png"
                     
alt="对比度" 
                ></td>
</tr>
<tr>
<td><strong>结构</strong></td>
<td><img 

                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/3765b0064f96885e010bb8eb3725455a.png"
                     
alt="结构" 
                >其中<img 

                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/6ba650fa5b1715045873b1422610d366.png"
                     
alt="协方差" 
                ></td>
</tr>
</tbody>
</table>
<p>结合即得到SSIM函数：</p>
<figure>
<img 

                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/fdd23e8d54ad6380ebc851cd06a0a12d.png"
                     
alt="SSIM" 
                >
<figcaption aria-hidden="true">SSIM</figcaption>
</figure>
<h3 id="lpips">LPIPS</h3>
<p>学习感知图像块相似度(Learned Perceptual Image Patch Similarity,
LPIPS)，通过深度学习模型来评估两个图像之间的感知差异。使用预训练的深度网络（如
VGG、AlexNet）来提取图像特征，然后计算这些特征之间的距离，以评估图像之间的感知相似度。</p>
<figure>
<img 

                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/e63001489b8b365f3bd6a46a08578664.jpeg"
                     
alt="LPIPS" 
                >
<figcaption aria-hidden="true">LPIPS</figcaption>
</figure>
<p>图示是将左右两幅图像与中间图像对比的结果。可以看到，传统方法（L2/PSNR,
SSIM,
FSIM）的结果与人的感知相反。而后三行通过神经网络提取特征的方法，能更符合人的感知，来评判图片的相似度。</p>
<h3 id="倒角距离">倒角距离</h3>
<p>出自：</p>
<blockquote>
<p>H. Fan, S. Hao, and L. Guibas, “A point set generation network for 3D
object reconstruction from a single image,” <a class="link" 
 href="https://so.csdn.net/so/search?q=CVPR&amp;spm=1001.2101.3001.7020" >CVPR<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>,
2017.</p>
</blockquote>
<figure>
<img 

                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/2c25811a255e9fb0702799148f56c8ed.png"
                     
alt="Chamfer distance" 
                >
<figcaption aria-hidden="true">Chamfer distance</figcaption>
</figure>
<p>算法：</p>
<ol type="1">
<li>对S1中任意一点x，计算它与S2中所有点的距离，取最小距离的平方。</li>
<li>遍历S1中的点，重复1中过程，求和所有距离平方。</li>
<li>同样的步骤，对S2中所有点遍历，重复1、2过程。</li>
<li>将两个求和结果相加，作为倒角距离。</li>
</ol>
<p>倒角距离用于衡量两个点云之间的相似度。如果该距离较大，则说明两组点云区别较大；如果距离较小，则说明重建效果较好。</p>
<h2 id="实验">实验</h2>
<h3 id="实验环境">实验环境</h3>
<ul>
<li><strong>Vanilla Gaussian Splatting</strong>: RTX 3090 * 1</li>
<li><strong>NeRF-based models</strong>: RTX 3090 * 4</li>
</ul>
<h3 id="实验结果">实验结果</h3>
<figure>
<img 

                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/3f83245b067971c4b84de5cd9895d823.png"
                     
alt="Table3" 
                >
<figcaption aria-hidden="true">Table3</figcaption>
</figure>
<ul>
<li>3DGS和SuGaR在图像的渲染中，有更优的效果（三个参数都更优），且训练时间更短。</li>
</ul>
<figure>
<img 

                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/b6069787b5c845e9080f4ef1d4751090.png"
                     
alt="Table4" 
                >
<figcaption aria-hidden="true">Table4</figcaption>
</figure>
<ul>
<li>NeRF: 使用ns-export生成3D点云。</li>
<li>3DGS: 每个Gaussian Splatting的均值作为一个点，输出点云。</li>
<li>神经辐射场生成的点云，通常包含很多与场景无关的异常值。</li>
<li>3DGS也存在边缘效应，边缘会模糊。</li>
</ul>
<figure>
<img 

                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/b9ceed006fcc21ee2617014e59db17c1.png"
                     
alt="CD" 
                >
<figcaption aria-hidden="true">CD</figcaption>
</figure>
<p>（图源项目网址：<a class="link" 
 href="https://saliteta.github.io/CUHKSZ_SMBU/" >GauU-Scene<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>）</p>
<ul>
<li>NeRFacto，虽然在图像生成得分最低，但是倒角距离最小。而Instant-NGP和SuGaR分别是倒数第一和倒数第二。这一实验结果揭示了基于图像的测量无法代表底层几何结构的基本事实。</li>
<li>SuGaR
是一种专为几何对齐设计的方法，排名却倒数。再对SuGaR进行定量分析，发现SuGaR在几何重建方面确实有更好的表现。</li>
</ul>
<figure>
<img 

                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/3472d29a30085a95fb9679362e2c9f49.png"
                     
alt="SuGaR" 
                >
<figcaption aria-hidden="true">SuGaR</figcaption>
</figure>
<p>图中的蓝点在其他方法中很常见，而绿色甚至略带红色的点在其他方法中却很少见。更何况SuGaR中绿色点如此之多。
从定量的角度来看，如果我们忽略这里显示的异常值，SuGaR
的确是最好的方法。</p>
<figure>
<img 

                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/0a3a7c7970e0d897124e5fa0d668a98d.png"
                     
alt="Figure6" 
                >
<figcaption aria-hidden="true">Figure6</figcaption>
</figure>
<p>对高斯泼溅的alpha值进行了简单分析，其中几乎三分之二的alpha值几乎是透明的。通过删除这些值，渲染的图像变得更加清晰，几乎没有信息丢失。这些近乎透明的高斯飞溅实例漂浮在3D空间中。尽管它们在渲染图像中看不到，但它们会导致几何测量指标的退化。</p>
]]></content>
      <categories>
        <category>三维重建</category>
      </categories>
      <tags>
        <tag>NeRF</tag>
        <tag>3DGS</tag>
      </tags>
  </entry>
  <entry>
    <title>Mujoco - CoACD简略教程</title>
    <url>/2024/09/29/3-CoACD_notes/</url>
    <content><![CDATA[<p>CoACD是一个凸分解工具，可以将凹模型粗略粗分解为凸几何体的并集。</p>
<h2 id="安装教程">安装教程</h2>
<h3 id="克隆代码">(1) 克隆代码：</h3>
<div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">git clone --recurse-submodules https://github.com/SarahWeiii/CoACD.git</span><br></pre></td></tr></table></figure></div>
<h3 id="安装依赖">(2) 安装依赖：</h3>
<div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">cmake &gt;= 3.24</span><br><span class="line">g++ &gt;= 9, &lt; 12</span><br></pre></td></tr></table></figure></div>
<blockquote>
<p>在我的Ubuntu22.04中，apt里的cmake包版本是3.22，不能用。用snap成功安装3.30版本。（源码安装好像也行，不过我懒得添加系统变量，就还是用snap安装了。</p>
</blockquote>
<h3 id="编译">(3) 编译</h3>
<div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">cd CoACD \</span><br><span class="line">&amp;&amp; mkdir build \</span><br><span class="line">&amp;&amp; cd build \</span><br></pre></td></tr></table></figure></div>
<p>​ 然后编译：</p>
<div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">cmake .. -DCMAKE_BUILD_TYPE=Release \</span><br><span class="line">&amp;&amp; make main -j</span><br></pre></td></tr></table></figure></div>
<blockquote>
<p>这里出了很多warning，但是好像不影响使用。</p>
</blockquote>
<h3 id="使用">(4) 使用</h3>
<div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">./main -i PATH_OF_YOUR_MESH -o PATH_OF_OUTPUT</span><br></pre></td></tr></table></figure></div>
<h2 id="参数说明">参数说明</h2>
<ul>
<li><strong>-nm/--no-merge</strong> : 禁用合并后处理，默认为false。</li>
<li><strong>-c/--max-convex-hull</strong> :
凸包上限，默认-1表示无限制。<strong>仅在启用合并时才</strong>有效。</li>
<li><strong>-ex/--extrude</strong> : 沿着重叠面挤出相邻的凸包。</li>
<li><strong>-am/--approximate-mode</strong> :
近似形状类型（“ch”表示凸包，“box”表示立方体）。</li>
<li><strong>--seed</strong> : 随机种子，默认是random()。</li>
</ul>
<p><strong>说明</strong>：</p>
<ol type="1">
<li>大多数情况下，只需调整<code>threshold</code>
（0.01~1）即可平衡细节程度和分解成分的数量。值越高，结果越粗，值越低，结果越细。</li>
<li>默认参数是快速版本。可以牺牲运行时间获取更多组件数量，增加<code>searching depth (-md)</code>
、
<code>searching node (-mn)</code>和<code>searching iteration (-mi)</code>可以获得更好的切割策略。</li>
</ol>
]]></content>
      <categories>
        <category>机器仿真</category>
      </categories>
      <tags>
        <tag>mujoco</tag>
        <tag>CoACD</tag>
        <tag>convex</tag>
      </tags>
  </entry>
  <entry>
    <title>NeRF和3DGS对比</title>
    <url>/2024/10/30/4-NeRFvs3DGS/</url>
    <content><![CDATA[<h2 id="nerf">NeRF</h2>
<h3 id="原理">原理</h3>
<p>NeRF（Neural Radiance
Fields）是一种新型的3D场景表示方法，通过神经网络来生成逼真的3D场景。NeRF
的核心思想是利用神经网络来表示一个场景的体素，从而可以实现从不同角度对场景进行渲染，生成高质量的图像。</p>
<p>特点：</p>
<ul>
<li>NeRF中三维模型的信息是以<strong>“隐式”</strong>的方法存储，而非点云、体素、网格等显式的表达方式。</li>
<li>NeRF使用类似<strong>光线追踪</strong>的方式创建新视角的图像。输入是采样点和观测的方向，输出是对应的RGB值和不透明度。</li>
</ul>
<h3 id="流程">流程</h3>
<figure>
<img 

                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/bcbfe7de8a71226465e395e9aca94b72.png"
                     
alt="Refer" 
                >
<figcaption aria-hidden="true">Refer</figcaption>
</figure>
<ol type="1">
<li><strong>输入信息</strong>：NeRF
将3D空间中的点（例如摄像机的位置）和观察方向作为输入。</li>
<li><strong>神经网络映射</strong>：通过一个MLP（多层感知器）神经网络，将每个3D坐标点和观察方向映射到颜色和体素密度。网络的输出包含颜色（RGB）和体密度（衡量光线穿过该点的透明度）。</li>
<li><strong>体渲染技术</strong>：NeRF
使用体渲染公式，即光线穿过场景的过程。具体来说，它沿着光线方向对多个点进行采样，并根据体密度和颜色计算每个点的贡献，进而得出最终的像素值。</li>
<li><strong>损失函数</strong>：通过多视图监督进行训练，NeRF会在场景中从多个视角捕获图像，将渲染的像素值与真实图像进行对比，并通过优化损失函数不断调整网络参数，逐步逼近真实场景。</li>
<li><strong>生成新视角</strong>：训练完成后，NeRF
能够在未见过的视角上生成图像，实现自由的3D视角切换和逼真的场景合成。</li>
</ol>
<h3 id="网络结构">网络结构</h3>
<figure>
<img 

                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/340d8d0e46ba24f221761300ece496f8.png"
                     
alt="network" 
                >
<figcaption aria-hidden="true">network</figcaption>
</figure>
<ul>
<li><strong>不透明度</strong>只和空间位置有关，<strong>颜色</strong>与空间位置和视角有关。</li>
</ul>
<h3 id="实现细节">实现细节</h3>
<h4 id="mlp网络">MLP网络</h4>
<ul>
<li><strong>层数</strong>：NeRF的网络由10个隐藏层组成，前9层包含256个神经元，最后一层包含128个神经元。</li>
<li><strong>分支结构</strong>：NeRF的MLP有一个特定的分支结构。前8层用于处理空间位置信息(x,y,z)，生成了隐变量（latent
features）。这些特征被用来预测体密度（density），以表示该点在场景中的不透明度。</li>
<li><strong>跳跃连接</strong>：位置编码后的输入会与网络中间层（第5层）相连，作为跳跃连接（skip
connection），帮助网络在深层结构中保持空间信息的细节。</li>
</ul>
<h4 id="位置编码">位置编码</h4>
<ul>
<li>NeRF
使用了傅里叶特征来将输入的3D坐标和视角方向进行编码，这种方式称为<strong>位置编码</strong>（Positional
Encoding）。具体来说，它将输入扩展到高频率空间，这样网络就能够学习更细腻的细节。</li>
<li>对于坐标 (x,y,z)(x, y, z)(x,y,z) 和方向
(θ,ϕ)，每个输入维度会生成多个不同频率的正余弦特征，以捕捉场景中的复杂空间结构。</li>
</ul>
<h4 id="体渲染">体渲染</h4>
<p>​
从焦点到一个像素上连的射线为<code>r(t)=o+td</code>，其中其中<code>o</code>是原点，<code>t</code>是距离。距离起点（near
bound）和距离终点（far bound）为<code>tn</code>和<code>tf</code>。</p>
<p>​ 获得像素颜色的公式：</p>
<p>​ <img 

                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/66ccede850b01c372315e2b505c4ec7b.png"
                     
alt="color" 
                ></p>
<p>​ 这个式子积分里面是<code>T(t)</code>
、<code>密度 σ(r(t))</code>和颜色<code>c(r(t),d)</code>的乘积，其中<code>T(t)</code>是累积透光率，表示光线射到这“还剩多少光”。实际渲染过程是把射线平均分成N个小区间，每个区间随机采样一个点，对采样得到的点的颜色进行某种加权求和。</p>
<h3 id="相关工作">相关工作</h3>
<h4 id="nerf2mesh">NeRF2Mesh</h4>
<figure>
<img 

                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/9e1ed1cebbf7c2079a3bd129333c9730.png"
                     
alt="NeRF2Mesh" 
                >
<figcaption aria-hidden="true">NeRF2Mesh</figcaption>
</figure>
<p>最左侧的立方体就是Nerf所构建的三维数据，它包含离散点的三维坐标、不透明度（density）以及rgb色彩。
Nerf2Mesh的整体结构也像图中分为上下两个分支：</p>
<ul>
<li><strong>密度分支 (Density
field)</strong>：首先利用NeRF生成的密度场划分出体素，再用Marching
Cubes生成三角网格，再进行优化。</li>
<li><strong>外观分支 (Appearance
Field)</strong>：Nerf输出的内容（RGB）经过MLP1提取特征，然后分成两个分支分别提取漫反射和镜面反射（镜面反射会多经过一个MLP层）的分量。</li>
</ul>
<h2 id="d-gaussian-splatting">3D Gaussian Splatting</h2>
<h3 id="原理-1">原理</h3>
<p>3D Gaussian Splatting 的核心思想是用高斯分布来表示 3D
空间中的点云，将场景中的点用 3D
高斯函数表示。这些高斯函数即“高斯球”，通过一组参数（均值、协方差矩阵等）来描述位置、方向、大小和形状。相比于单纯的点云，使用高斯分布可以更好地对点的位置和形状进行逼真地表达，使得结果更平滑并且抗噪性更好。</p>
<p>在渲染过程中，场景以相机视角来观察这些高斯分布的点，生成一个图像。每个像素的颜色和透明度是通过聚合沿视线方向的高斯球信息来计算的。这些高斯分布产生的重叠区域，通过数学上的加权平均可以很好地呈现出自然的模糊边缘，避免了传统点云中颗粒状的视觉问题。</p>
<h3 id="流程-1">流程</h3>
<figure>
<img 

                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/b957eb22052d10a72d8d78c8416a0af4.png"
                     
alt="3DGS" 
                >
<figcaption aria-hidden="true">3DGS</figcaption>
</figure>
<ol type="1">
<li><strong>点云采样</strong>：使用SfM从一组图像中估计出点云，可以直接调用
<a class="link" 
 href="https://link.zhihu.com/?target=https%3A//colmap.github.io/" >COLMAP<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>
库操作</li>
<li><strong>高斯分布</strong>：将每个点分配一个高斯分布。初始高斯分布通过位置、颜色、透明度和协方差矩阵等参数来定义。</li>
<li><strong>投影变换</strong>：通过投影操作将 3D
高斯分布映射到相机视角下的 2D 图像平面。</li>
<li><strong>光栅化</strong>：对投影后的高斯分布进行渲染。</li>
<li><strong>优化</strong>：通过计算图像与目标图像之间的损失，反向传播梯度（Gradient
Flow）来调整高斯参数。以及自适应地调整高斯分布的密度。</li>
</ol>
<h3 id="实现细节-1">实现细节</h3>
<h4 id="三维高斯属性">三维高斯属性</h4>
<p>用三维高斯分布构建基础元素，属性有中心<code>μ</code>、不透明度<code>α</code>、三维协方差矩阵（表示缩放程度）<code>Σ</code>和颜色<code>c</code>。其中颜色<code>c</code>与视角有关，由球谐函数表示。</p>
<p>球谐函数的示例：</p>
<figure>
<img 

                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/2b098bcc4d5038318f72a2e4fb51ef3b.jpeg"
                     
alt="球谐函数" 
                >
<figcaption aria-hidden="true">球谐函数</figcaption>
</figure>
<p>（图源：<a class="link" 
 href="https://zhuanlan.zhihu.com/p/679809915" >3DGS综述以及对3DGS的理解：A
Survey on 3D Gaussian Splatting - 知乎<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>）</p>
<h4 id="自适应密度控制">自适应密度控制</h4>
<figure>
<img 

                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/d8d3a1e3a41d17eeebed3b751bf58459.png"
                     
alt="自适应密度控制" 
                >
<figcaption aria-hidden="true">自适应密度控制</figcaption>
</figure>
<ul>
<li><strong>Under-Reconstruction</strong>:
复制一个当前高斯分布的副本，然后沿着位置梯度方向移动它。</li>
<li><strong>Over-Reconstruction</strong>:
将当前高斯分布分割成两个较小的高斯分布，再对其进行移动。</li>
</ul>
<h4 id="点的剪枝">点的剪枝</h4>
<p>对于冗余的高斯分布，在迭代过程中会逐渐消除。不透明度太低的高斯分布和过大的高斯分布都会在迭代中逐渐消除。以节省资源。</p>
<h4 id="tile">Tile</h4>
<p>为了降低运算成本，3DGS将图像分割为数个不重叠的patch，称为<code>tile</code>，每个<code>tile</code>为<code>16×16</code>像素。在此基础上，3DGS计算投影后的高斯与<code>tile</code>的相交情况。由于高斯可能与多个<code>tile</code>相交，所以对其进行了复制，并为其分配<code>tile</code>的标识符。不同的<code>tile</code>可以在不同的线程或
GPU 核心上同时计算，避免了对同一像素的竞争。</p>
<h2 id="对比">对比</h2>
<h3 id="场景绘制">场景绘制</h3>
<ul>
<li>NeRF使用光线追踪来创建新视点的图像。通过模拟从像素位置发射的光线来计算图像中每个像素的颜色，运行速度较慢。但渲染更真实。</li>
<li>3DGS
使用光栅化来创建新视点的图像，通过Splatting进行渲染，运算速度更快，但3DGS创建了数百万个gaussion，占用的内存比NeRF多数倍。</li>
</ul>
<h3 id="d重建">3D重建</h3>
<ul>
<li>NeRF的输出需要借助其他方法转为显式输出，如NeRF2Mesh。</li>
<li>3DGS训练后最终得到的是一个文件，其中包含每个高斯的高斯参数列表，例如位置、大小、颜色和透明度。3DGS的输出是比较显式的表达。</li>
</ul>
<p>在CSDN上有一个3DGS和NeRF的对比表格，但数据来源并未标明，可靠性存疑（而且他的3DGS怎么是3D
Geometry Sensing的缩写，虽然他给的结构图确实是3D高斯的）：</p>
<table>
<colgroup>
<col style="width: 11%" />
<col style="width: 44%" />
<col style="width: 44%" />
</colgroup>
<thead>
<tr>
<th><strong>对比维度</strong></th>
<th>3DGS (3D Geometry Sensing)</th>
<th>NeRF (Neural Radiance Fields)</th>
</tr>
</thead>
<tbody>
<tr>
<td><strong>基本原理</strong></td>
<td>基于几何推断，通过多视角图像、深度传感器、LiDAR等获取显式3D几何信息。</td>
<td>基于神经网络拟合体积辐射场，通过多视角图像学习隐式表示，渲染出场景。</td>
</tr>
<tr>
<td><strong>输入数据</strong></td>
<td>多视角图像、深度信息（LiDAR、ToF相机）、位姿数据、点云。</td>
<td>多视角图像（通常包括相机位姿），不需要显式的几何信息。</td>
</tr>
<tr>
<td><strong>输出结果</strong></td>
<td>点云、网格、三角形模型、深度图、纹理映射等显式几何结构。</td>
<td>通过体积渲染生成逼真图像（视角相关），不直接输出几何模型。</td>
</tr>
<tr>
<td><strong>数据处理方式</strong></td>
<td>使用几何关系（如三角测量、立体视觉等）来显式重建场景结构。</td>
<td>使用神经网络隐式建模颜色和密度，通过体积渲染生成图像。</td>
</tr>
<tr>
<td><strong>几何信息</strong></td>
<td>显式获取3D几何信息，可以精确测量物体的距离和形状。</td>
<td>隐式推断几何信息，主要用于图像渲染，几何结构不直接输出。</td>
</tr>
<tr>
<td><strong>渲染效果</strong></td>
<td>依赖于重建的几何结构，渲染效果有限，尤其在复杂光线场景下效果一般。</td>
<td>渲染效果非常逼真，尤其在反射、遮挡、折射等复杂光照场景表现优异。</td>
</tr>
<tr>
<td><strong>计算资源需求</strong></td>
<td>需要较强的几何计算能力，数据获取通常依赖于多传感器系统（LiDAR等）。</td>
<td>需要高计算资源，特别是训练神经网络的过程计算量大，通常依赖于GPU。</td>
</tr>
<tr>
<td><strong>渲染速度</strong></td>
<td>实时性较好，特别是有深度传感器时可实现快速重建。</td>
<td>渲染速度较慢，尤其在训练阶段耗时长，但有即时渲染版本。</td>
</tr>
<tr>
<td><strong>应用场景</strong></td>
<td>自动驾驶、机器人导航、工业检测、3D建模、AR/VR、精密测量。</td>
<td>电影视觉特效、虚拟旅游、虚拟现实内容生成、复杂光照场景的渲染。</td>
</tr>
<tr>
<td><strong>优点</strong></td>
<td>可以显式建模、精确几何测量、适用于实时应用；深度传感器辅助时重建精度高。</td>
<td>渲染质量极高，能处理复杂的光照、遮挡问题；不需要昂贵的深度传感器。</td>
</tr>
<tr>
<td><strong>缺点</strong></td>
<td>在处理复杂光照（如透明物体、反射面）时效果不佳，依赖昂贵的传感器数据。</td>
<td>渲染速度慢，训练时间长，初始设计不擅长生成明确的几何信息。</td>
</tr>
<tr>
<td><strong>几何建模精度</strong></td>
<td>高，适合用于需要精确几何信息的场景（如测量、导航、物理模拟等）。</td>
<td>几何建模是隐式的，主要依赖于神经网络推断，不适合用于测量等任务。</td>
</tr>
<tr>
<td><strong>光照处理</strong></td>
<td>处理复杂光线条件较困难，通常需要额外的算法来应对光线反射和折射。</td>
<td>对复杂光照场景处理效果出色，能够处理反射、折射、遮挡等问题。</td>
</tr>
<tr>
<td><strong>实时性</strong></td>
<td>实时性较强，特别是在配合LiDAR等传感器时。</td>
<td>需要较长的时间进行训练和渲染，不适合实时应用（加速版本除外）。</td>
</tr>
<tr>
<td><strong>数据获取成本</strong></td>
<td>高，需要多视角相机或昂贵的深度传感器（如LiDAR）。</td>
<td>低，仅需多视角图像数据，不依赖于专门的传感器。</td>
</tr>
</tbody>
</table>
<p>（表源：<a class="link" 
 href="https://blog.csdn.net/Darlingqiang/article/details/142773142" >【3dgs】3DGS与NeRF对比_nerf和3dgs区别-CSDN博客<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>）</p>
]]></content>
      <categories>
        <category>三维重建</category>
      </categories>
      <tags>
        <tag>NeRF</tag>
        <tag>3DGS</tag>
      </tags>
  </entry>
  <entry>
    <title>SplatSim 论文解读</title>
    <url>/2024/11/06/6-SplatSim/</url>
    <content><![CDATA[<h2 id="背景">背景</h2>
<p>Sim2Real是机器人技术中的一个核心问题，涉及将模拟环境中学到的控制策略转移到现实世界环境中。目前的方法基本都依赖于深度、触觉传感或点云输入等感知方式。相比之下，RGB
图像很少用作机器人学习应用中的主要传感方式。优于 Sim2Real
传输中的其他常用方式。它们捕捉关键的视觉细节，例如颜色、纹理、照明和表面反射率等，这对于理解复杂的环境至关重要。此外，RGB
图像很容易在现实环境中使用相机获取，并且与人类感知紧密结合，使其非常适合解释动态和复杂场景中的复杂细节。</p>
<p>为什么很难将使用 RGB
信息进行模拟训练的策略部署到现实世界呢？是因为机器人在模拟器中观察到的图像分布与它在现实世界中看到的图像分布有很大不同。本文提出了一种新颖的方法来减少
RGB 图像的 Sim2Real 差距。利用 Gaussian Splatting
作为照片级真实感渲染，使用现有模拟器作为物理主干。利用 Gaussian
Splatting
作为主要渲染基元，取代现有模拟器中传统的基于网格的表示，以显著提高渲染场景的照片真实感。</p>
<h2 id="方法">方法</h2>
<ul>
<li><strong>关键前提</strong>：准确分割现实场景中高斯分布表示的每个刚体，并识别其相对于模拟器的相应的齐次变换。那么就可以渲染新姿势下的刚体。</li>
<li><strong>底层表示</strong>：不使用网格图元，而是使用高斯图作为底层表示。</li>
</ul>
<h3 id="a.-问题描述">A. 问题描述</h3>
<p><span class="math inline">\(\mathcal{S}_{real}\)</span>
表示真实场景的Gaussian Splat。 <span
class="math inline">\(\mathcal{S}^{k}_{obj}\)</span>
表示场景中第k个object的Gaussian Splat。
目标是为任何模拟器中的机器人，使用 <span
class="math inline">\(\mathcal{S}_{real}\)</span> 来生成真实的渲染 <span
class="math inline">\(\mathcal{I}^{sim}\)</span>
。然后在这样的表示下，收集专家的演示 <span
class="math inline">\(ε\)</span>​​ 来用于训练基于RGB的策略。</p>
<p>专家 <span class="math inline">\(ε\)</span> 生成由状态-动作对<span
class="math inline">\(\{(s_1,a_1),\ldots,(s_T,a_T)\}\)</span>组成的轨迹
<span class="math inline">\(τ_ε\)</span> 。每个时间步 <span
class="math inline">\(t\)</span> 的状态定义为 <span
class="math inline">\(s_{t}=(q_{t},x_{t}^{1},\ldots,x_{t}^{n})\)</span>
。其中 <span class="math inline">\(q_t\in\mathbb{R}^m\)</span>
代表机器人的关节角度。<span
class="math inline">\(x_t^k=(p_t^k,R_t^k)\)</span> 表示第 <span
class="math inline">\(k\)</span> 个 object 的位置 <span
class="math inline">\(p_{t}^{k}\in\mathbb{R}^{3}\)</span> 和方向 <span
class="math inline">\(R_{t}^{k} \in SO(3)\)</span> 。对应的动作 <span
class="math inline">\(a_t = (p_t^e,R_t^{e})\)</span> 指末端执行器的位置
<span class="math inline">\(p_{t}^{e}\in\mathbb{R}^{3}\)</span> 和方向
<span class="math inline">\(R_{t}^{e}\in SO(3)\)</span> 。</p>
<p>渲染图 <span class="math inline">\(I^{sim}\)</span> ，由模拟状态
<span class="math inline">\(s_{t}\)</span> 推导而来，作为输入训练策略
<span class="math inline">\(\pi_{\tau}\)</span>
。测试时，该策略只依赖于现实世界的RGB图像 <span
class="math inline">\(I^{real}\)</span> 。</p>
<h3 id="b.-坐标系定义和变换">B. 坐标系定义和变换</h3>
<p><span
class="math inline">\(\mathcal{F}_{real}\)</span>：真实世界坐标系 -
主要参考系。</p>
<p><span
class="math inline">\(\mathcal{F}_{robot}\)</span>：真实世界机器人坐标系。</p>
<p><span
class="math inline">\(\mathcal{F}_{sim}\)</span>：模拟器坐标系。</p>
<p><span class="math inline">\(\mathcal{F}_{robot}\)</span> 和 <span
class="math inline">\(\mathcal{F}_{sim}\)</span> 都会与 <span
class="math inline">\(\mathcal{F}_{real}\)</span>
进行对齐，以确保模拟器中机器人底座和现实世界共享相同的坐标系。</p>
<h3 id="c.-机器人-splat-模型">C. 机器人 Splat 模型</h3>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/1c7c7e02cbb7f0779e69c9b28f154dd4.png"
                      alt="coords" style="zoom: 67%;" 
                ></p>
<p>首先创建场景的高斯分布（其中机器人位于其原始位置），在静态场景中对机器人进行可视化。使用
ICP
算法手动分割机器人的点云并与标准机器人框架对齐。然后对每个机器人关节进行分段，并应用正向运动学变换，从而能够以任意关节配置渲染机器人。</p>
<h3 id="d.-object-splat模型">D. Object Splat模型</h3>
<p>与机器人渲染类似，使用 ICP 来对齐每个对象的 3D 高斯 <span
class="math inline">\(\mathcal{S}^{k}_{obj}\)</span>​​
到其模拟的真实点云。 <span class="math display">\[
T=(T_{\mathcal{F}_{robot}}^{\mathcal{F}_{splat}})^{-1}\cdot
T_{fk}^{k-obj}\cdot
T_{\mathcal{F}_{k-obj,sim}}^{\mathcal{F}_{k-obj,splat}}
\]</span></p>
<h3 id="e.-连接的物体">E. 连接的物体</h3>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/12feffa9ebb35ef048fd028dce32ab5d.png"
                      alt="Articulated Object" style="zoom:67%;" 
                ></p>
<p>虽然 CAD
轴对齐的边界框允许对机器人连杆进行直接分割，但某些物体（例如平行钳口夹具）由于与标准轴未对准而带来了挑战，也就是说，仅使用边界框无法将夹具连杆整齐地分割出来。文章使用基于
KNN 的分类器对平行颚式夹具等铰接物体的链接进行分段。</p>
<h3 id="f.-使用-splatsim-渲染模拟轨迹">F. 使用 SplatSim
渲染模拟轨迹</h3>
<p>既然已经能够在场景中渲染单个刚体，那么可以用它来表示任何模拟轨迹
<span class="math inline">\(T_{\epsilon}\)</span>
。用这些基于状态的转换和C、D里提到的方法，获得演示，从而让策略从 <span
class="math inline">\(\tau_{\mathcal{G}}=\{(I_{1}^{sim},a_{1}),(I_{2}^{sim},a_{2}),\ldots,(I_{T}^{sim},a_{T})\}\)</span>
进行学习。</p>
<h3 id="g.-策略训练和部署">G. 策略训练和部署</h3>
<p>为了在模拟器中从生成的演示 <span
class="math inline">\(\tau_{\mathcal{G}}\)</span>​ 中学习
，采用扩散策略。尽管论文的方法显著缩小了 Sim2Real
视觉上的差距，但模拟环境和现实环境之间的差异仍然存在。例如，模拟场景缺乏阴影，刚体的假设会导致机器人电缆等可动部件的渲染不当。为了解决这些问题，论文在策略训练期间结合了图像增强，其中包括添加高斯噪声、随机擦除以及调整图像的亮度和对比度。这些增强显著增强了策略的稳健性并提高了其在实际部署过程中的性能。</p>
<h2 id="实验">实验</h2>
<h3 id="a.-现实世界和模拟中的演示">A. 现实世界和模拟中的演示</h3>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/5c1e1f25c182fd193d795ada326b63b3.png"
                      alt="Comparation" style="zoom:67%;" 
                ></p>
<p>在现实世界中，每项任务的演示都是由人类专家手动收集的。
相比之下，模拟器通过采用基于特权信息的运动规划器简化了这一过程，运动规划器利用特权信息自动生成数据，例如场景中每个刚体的位置和方向。
在有人类专家参与的情况下，模拟器不仅能在演示之间自动重置，从而减少工作量，更重要的是，它利用运动规划器，完全消除了人类干预的需要。这样，只需极少的人工输入，就能生成大规模、高质量的演示数据集。
因此，模拟器大大减少了数据收集所需的时间和精力。 如表 I
所示，在现实世界中收集演示数据需要约 20.5 个小时，而在模拟器中只需 3
个小时就能完成同样的任务，这充分体现了方法的高效性和可扩展性。</p>
<h3 id="b.-零样本策略部署结果">B. 零样本策略部署结果</h3>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/3469ddcbe24f2849e73ac7001682e5f6.png"
                      alt="Deployment Results" style="zoom:67%;" 
                ></p>
<p>以任务成功率为主要指标，评估了策略在四个接触丰富的真实世界任务中的部署情况。
如表 I 所示，方法实现了 86.25% 的 Sim2Real
传输平均成功率，而直接在真实世界数据上训练的策略成功率为
97.5%，这凸显了方法的有效性。 所有实验均使用配备了 Robotiq 2F-85 抓手和
2 个英特尔 Realsense D455 摄像头的 UR5 机器人，并在英伟达 RTX 3080Ti GPU
上部署了扩散策略。</p>
<ol type="1">
<li>T-Push 任务： T-Push 任务由 Diffusion
Policy推广，可捕捉非触觉操作的动态，其中涉及控制物体移动和接触力。
在训练中，人类专家使用 Gello teleoper-ation，在模拟中收集了 160 次演示。
测试时，机器人从随机位置出发，在零样本 Sim2Real 传输中取得了 90%
的成功率（36/40 次试验），如表 I 所示。
这一结果表明，框架在真实世界的演示中无需微调就能有效处理推动的动态过程。
此外，方法与 Real2Real（40/40）和 Sim2Sim（40/40）的性能相当。</li>
<li>Pick-Up-Apple 任务： Pick-Up-Apple
任务涉及在三维空间中抓取和操纵物体的完整姿态（即位置和方向）。
该任务旨在评估机器人在使用论文的模拟渲染场景进行训练时的抓取能力。
运动规划器利用模拟器中的特权状态信息（场景中每个刚体的准确位置和方向），生成了
400 个具有随机末端执行器位置和方向的演示。 如表 I
所示，在真实世界的试验中，策略在零样本 Sim2Real 传输中取得了 95%
的成功率（38/40 次试验）。</li>
<li>Orange on Plate 任务：
在这项任务中，机器人必须捡起一个橘子并将其放在盘子里。
在模拟过程中，运动规划器获取了特权信息，并生成了 400 次演示。
在训练过程中，末端执行器的位置和初始抓手状态是随机的。
测试期间，机器人总是从原点开始。 论文在 Sim2Real 的零点转移中取得了 90%
的成功率（36/40 次试验）。</li>
<li>Assembly 任务：
在这项任务中，机器人必须将一个长方体块放在另一个长方体块的顶部。
机器人从原点开始抓取绿色立方体，并将其放到红色立方体的顶部。
这项任务特别艰巨，因为机器人必须精确放置，否则立方体就会掉落，导致失败。
论文的 Sim2Real 策略在这项任务中的表现为 70%（28/40 次试验），而 Sim2Sim
的表现为 95%，Real2Real 的表现为 90%。</li>
</ol>
<h3 id="c.-量化机器人渲染">C. 量化机器人渲染</h3>
<p>通过与真实世界的图像进行比较，论文定量评估了在不同关节配置下渲染的机器人图像的准确性。
论文评估了 300 个不同机器人关节角度下的机器人渲染质量。
为了衡量渲染图像与真实世界图像之间的相似性，论文采用了图像渲染评估中常用的两个指标：
峰值信噪比（PSNR）和结构相似性指数（SSIM）。
尽管关节配置各不相同，但渲染图像的平均 PSNR 和 SSIM 分别达到了 22.62 和
0.7845，表明模拟图像非常接近真实世界 RGB 观察图像的视觉质量。</p>
<h3 id="d.-数据增强的效果">D. 数据增强的效果</h3>
<p>为了量化数据增强对策略在模拟与真实环境中性能的影响，论文对经过训练的策略进行了有增强和无增强的对比实验。
虽然在一致的环境（如 Sim2Sim 或 Real2Real
场景）中，扩散策略在没有增强的情况下也能有效执行，但将在模拟环境中训练的策略转移到真实世界时，由于渲染无法捕捉动态细节（如不断变化的反射和阴影），因此会引入领域偏移，从而需要额外的鲁棒性。
论文在训练过程中加入了随机噪音添加、色彩抖动和随机擦除等增强功能，以应对这些变化。
在 B 节的四项任务中，这些增强措施将该策略的性能从 21% 提高到
86.25%。</p>
<h2 id="结论">结论</h2>
<p>在这项工作中，论文利用高斯泼溅技术（Gaussian
Splatting）作为一种逼真的渲染技术，并与现有的基于物理交互的模拟器集成，从而缩小了基于
RGB 的操作策略的模拟与真实之间的差距。
论文的框架实现了在模拟中训练好的基于 RGB
的操作策略到真实环境中的零样本转移。
虽然论文的框架推动了当前最先进技术的发展，但它仍局限于刚体操纵，无法处理布、液体或植物等复杂物体。
未来计划将现有的框架与基于强化学习的方法相结合，以获得更多动态技能。还将进一步改进系统，以便在现实世界中训练和部署机器人执行高度复杂和接触丰富的任务。</p>
]]></content>
      <categories>
        <category>三维重建</category>
      </categories>
      <tags>
        <tag>3DGS</tag>
        <tag>Sim2Real</tag>
      </tags>
  </entry>
  <entry>
    <title>Mathjax与渲染引擎marked冲突解决方案</title>
    <url>/2024/11/11/7-Build_blog/</url>
    <content><![CDATA[<h3 id="问题说明">问题说明</h3>
<p><code>hexo</code>在解析markdown的时候，会对一些符号如<code>_</code>进行转义，将其转为<code>&lt;em&gt;</code>标签。而在公式块中，<code>_</code>是作为渲染下标所用的符号。但是<code>hexo</code>的优先级比<code>mathjax</code>更高，且不会判断<code>_</code>是否在公式内，所以很容易造成冲突，导致公式没法正常渲染。比如笔者在编写博客时，发现两个相邻公式中的<code>_</code>被当成斜体渲染，导致两个公式都没正常渲染出来。</p>
<h3 id="踩坑过程">踩坑过程</h3>
<h4 id="修改源码失败">1.修改源码（失败）</h4>
<p>太长了，折叠一下，想看就展开吧👇。</p>
<details class="orange" data-header-exclude><summary><i class="fa-solid fa-chevron-right"></i>Folding 点击查看更多 </summary>
              <div class='content'>
              <p>搜到的大部分解决方案，都是让更改marked源码，即将<code>nodes_modules/lib/marked/lib/marked.js</code>文件进行更改：</p><p>将</p><div class="code-container" data-rel="Js"><figure class="iseeu highlight js"><table><tr><td class="code"><pre><span class="line"><span class="attr">escape</span>: <span class="regexp">/^\\([\\`*&#123;&#125;\[\]()# +\-.!_&gt;])/</span>,</span><br></pre></td></tr></table></figure></div><p>替换为</p><div class="code-container" data-rel="Js"><figure class="iseeu highlight js"><table><tr><td class="code"><pre><span class="line"><span class="attr">escape</span>: <span class="regexp">/^\\([`*&#123;&#125;\[\]()# +\-.!_&gt;])/</span>,</span><br></pre></td></tr></table></figure></div><p>从而去除<code>\\</code>的转义。</p><p>将</p><div class="code-container" data-rel="Js"><figure class="iseeu highlight js"><table><tr><td class="code"><pre><span class="line"><span class="attr">em</span>: <span class="regexp">/^\b_((?:[^_]|__)+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/</span>,</span><br></pre></td></tr></table></figure></div><p>替换为</p><div class="code-container" data-rel="Js"><figure class="iseeu highlight js"><table><tr><td class="code"><pre><span class="line"><span class="attr">em</span>:<span class="regexp">/^\*((?:\*\*|[\s\S])+?)\*(?!\*)/</span>,</span><br></pre></td></tr></table></figure></div><p>从而去除<code>_</code>的斜体转义。</p><p>然而不知道是博主使用的版本问题还是什么原因，并没有找到该文件。虽然后面在<code>nodes_modules/marked/bin</code>里找到了<code>marked.js</code>，它通过调用了一个库<code>marked.esm.js</code> 来执行具体的 Markdown解析操作。因此博主在<code>marked.esm.js</code>进行了相应的更改。</p><p>将</p><div class="code-container" data-rel="Js"><figure class="iseeu highlight js"><table><tr><td class="code"><pre><span class="line"><span class="attr">emStrong</span>: &#123;</span><br><span class="line">  <span class="attr">lDelim</span>: <span class="regexp">/^(?:\*+(?:([punct_])|[^\s*]))|^_+(?:([punct*])|([^\s_]))/</span>,</span><br><span class="line"> </span><br><span class="line">  <span class="comment">//        (1) and (2) can only be a Right Delimiter. (3) and (4) can only be Left.  (5) and (6) can be either Left or Right.</span></span><br><span class="line">  <span class="comment">//          () Skip orphan inside strong                                      () Consume to delim     (1) #***                (2) a***#, a***                             (3) #***a, ***a                 (4) ***#              (5) #***#                 (6) a***a</span></span><br><span class="line">  <span class="attr">rDelimAst</span>: <span class="regexp">/^(?:[^_*\\]|\\.)*?\_\_(?:[^_*\\]|\\.)*?\*(?:[^_*\\]|\\.)*?(?=\_\_)|(?:[^*\\]|\\.)+(?=[^*])|[punct_](\*+)(?=[\s]|$)|(?:[^punct*_\s\\]|\\.)(\*+)(?=[punct_\s]|$)|[punct_\s](\*+)(?=[^punct*_\s])|[\s](\*+)(?=[punct_])|[punct_](\*+)(?=[punct_])|(?:[^punct*_\s\\]|\\.)(\*+)(?=[^punct*_\s])/</span>,</span><br><span class="line">  <span class="attr">rDelimUnd</span>: <span class="regexp">/^(?:[^_*\\]|\\.)*?\*\*(?:[^_*\\]|\\.)*?\_(?:[^_*\\]|\\.)*?(?=\*\*)|(?:[^_\\]|\\.)+(?=[^_])|[punct*](\_+)(?=[\s]|$)|(?:[^punct*_\s\\]|\\.)(\_+)(?=[punct*\s]|$)|[punct*\s](\_+)(?=[^punct*_\s])|[\s](\_+)(?=[punct*])|[punct*](\_+)(?=[punct*])/</span> <span class="comment">// ^- Not allowed for _</span></span><br><span class="line">&#125;,</span><br></pre></td></tr></table></figure></div><p>替换为</p><div class="code-container" data-rel="Js"><figure class="iseeu highlight js"><table><tr><td class="code"><pre><span class="line"><span class="attr">emStrong</span>: &#123;</span><br><span class="line">  <span class="comment">// 仅匹配 * 而不匹配 _</span></span><br><span class="line">  <span class="attr">lDelim</span>: <span class="regexp">/^(?:\*+(?:([punct*])|[^\s*]))/</span>, </span><br><span class="line">  <span class="comment">// 保持原有的 * 匹配</span></span><br><span class="line">  <span class="attr">rDelimAst</span>: <span class="regexp">/^(?:[^_*\\]|\\.)*?\_\_(?:[^_*\\]|\\.)*?\*(?:[^_*\\]|\\.)*?(?=\_\_)|(?:[^*\\]|\\.)+(?=[^*])|[punct_](\*+)(?=[\s]|$)|(?:[^punct*_\s\\]|\\.)(\*+)(?=[punct_\s]|$)|[punct_\s](\*+)(?=[^punct_\s])|[\s](\*+)(?=[punct_])|[punct_](\*+)(?=[punct_])|(?:[^punct*_\s\\]|\\.)(\*+)(?=[^punct*_\s])/</span>,</span><br><span class="line">  <span class="comment">// 禁用对下划线的右界定符匹配</span></span><br><span class="line">  <span class="attr">rDelimUnd</span>: <span class="literal">null</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></div><p>然而后面看到该文件开头有这样一句：</p><div class="code-container" data-rel="Js"><figure class="iseeu highlight js"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * DO NOT EDIT THIS FILE</span></span><br><span class="line"><span class="comment"> * The code in this file is generated from files in ./src/</span></span><br><span class="line"><span class="comment"> */</span></span><br></pre></td></tr></table></figure></div><p>好嘛，原来这个文件是通过其他源文件生成的，于是又转头更改<code>nodes_modules/marked/src/rules.js</code>。同上面的规则更改<code>emStrong</code>。</p><p>然而，并无卵用。基本宣告这条路失败。</p>
              </div>
            </details>
<h4 id="手动escape成功但丑陋">2.手动escape（成功但丑陋）</h4>
<p>最简单的方法，博主最初的妥协。</p>
<p>就是将公式中的<code>_</code>前面加上<code>\</code>进行转义。这样的结果就是网站上能正常显示，然而用自己的markdown编辑软件，下标就会消失，变成真正的下横线。如果单纯只对网站更改，这个方法无疑是最方便的，但是但凡要在本地编辑，就会很难受。</p>
<h4 id="更换kramed引擎没用">3.更换kramed引擎（没用）</h4>
<p>直接卸载源引擎，更换kramed：</p>
<div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">npm uninstall hexo-renderer-marked --save</span><br><span class="line">npm install hexo-renderer-pandoc --save</span><br></pre></td></tr></table></figure></div>
<p>然而kramed引擎还是存在相同的问题。（所以为什么要让我改啊！改了还容易报一堆错）</p>
<p>还是需要修改<code>node_modules/kramed/lib/rules/inline.js</code>文件。</p>
<p>将</p>
<div class="code-container" data-rel="Js"><figure class="iseeu highlight js"><table><tr><td class="code"><pre><span class="line"><span class="attr">em</span>: <span class="regexp">/^\b_((?:__|[\s\S])+?)_\b|^\*((?:\*\*|[\s\S])+?)\*(?!\*)/</span>,</span><br></pre></td></tr></table></figure></div>
<p>更改为</p>
<div class="code-container" data-rel="Js"><figure class="iseeu highlight js"><table><tr><td class="code"><pre><span class="line"><span class="attr">em</span>: <span class="regexp">/^\*((?:\*\*|[\s\S])+?)\*(?!\*)/</span>,</span><br></pre></td></tr></table></figure></div>
<p>将</p>
<div class="code-container" data-rel="Js"><figure class="iseeu highlight js"><table><tr><td class="code"><pre><span class="line"><span class="attr">escape</span>: <span class="title function_">replace</span>(inline.<span class="property">escape</span>)(<span class="string">&#x27;])&#x27;</span>, <span class="string">&#x27;~|])&#x27;</span>)(),</span><br></pre></td></tr></table></figure></div>
<p>更改为</p>
<div class="code-container" data-rel="Js"><figure class="iseeu highlight js"><table><tr><td class="code"><pre><span class="line"><span class="attr">escape</span>: <span class="title function_">replace</span>(inline.<span class="property">escape</span>)(<span class="string">&#x27;])&#x27;</span>, <span class="string">&#x27;~])&#x27;</span>)(),</span><br></pre></td></tr></table></figure></div>
<p>这不是和上面一样的吗，而且博主依然没找到这个文件。不知道是不是版本迭代更改了结构。</p>
<h3 id="解决方案">解决方案</h3>
<p>最终的解决方案，是博主最开始最不愿意的方案，就是替换成pandoc引擎。</p>
<ol type="1">
<li><p>安装Pandoc。</p>
<p>在官网<a class="link"   href="https://pandoc.org/installing.html" >Pandoc -
Installing
pandoc<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>下载安装<code>pandoc</code>，博主是在<code>windows</code>上部署的博客，直接下载了msi安装包进行安装，安装完成后重启电脑后才生效。</p></li>
<li><p>更换引擎。</p>
<div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">npm uninstall hexo-renderer-marked --save</span><br><span class="line">npm install hexo-renderer-pandoc --save</span><br></pre></td></tr></table></figure></div>
<p>mathjax的安装和配置可参考网上的教程。</p></li>
<li><p>重新构建。</p>
<div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">hexo cl</span><br><span class="line">hexo d -g</span><br></pre></td></tr></table></figure></div></li>
</ol>
<p>然后应该就能正常显示公式了，这下舒服了。不得不说，<code>pandoc</code>，你是我的神！</p>
]]></content>
      <categories>
        <category>博客搭建</category>
      </categories>
      <tags>
        <tag>blog</tag>
        <tag>mathjax</tag>
        <tag>marked</tag>
      </tags>
  </entry>
  <entry>
    <title>NeRF Studio简要教程</title>
    <url>/2024/11/12/8-NeRF_Studio/</url>
    <content><![CDATA[<h2 id="准备工作">准备工作</h2>
<h3 id="安装nerf-studio">安装NeRF Studio</h3>
<p><a class="link" 
 href="https://github.com/nerfstudio-project/nerfstudio?tab=readme-ov-file" >官方仓库<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>
写的教程已经很详尽了。</p>
<div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/nerfstudio-project/nerfstudio.git</span><br><span class="line"><span class="built_in">cd</span> nerfstudio</span><br><span class="line">pip install --upgrade pip setuptools</span><br><span class="line">pip install -e .</span><br></pre></td></tr></table></figure></div>
<p>值得注意的是，<code>open3d</code>库只支持<code>python 3.8-3.11</code>，博主是用<code>python 3.10</code>安装的依赖。后面租了个服务器用<code>python 3.12</code>，结果找不到相应版本的open3d，建议还是按推荐配置来。</p>
<h3 id="安装tiny-cuda-nn">安装tiny-cuda-nn</h3>
<p>在训练过程中，终端出现了如下的warning：</p>
<div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">WARNING: Using a slow implementation <span class="keyword">for</span> the SHEncoding module. </span><br><span class="line">🏃 🏃 Install tcnn <span class="keyword">for</span> speedups 🏃 🏃</span><br><span class="line">pip install git+https://github.com/NVlabs/tiny-cuda-nn/#subdirectory=bindings/torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">WARNING: Using a slow implementation <span class="keyword">for</span> the NeRFEncoding module. </span><br><span class="line">🏃 🏃 Install tcnn <span class="keyword">for</span> speedups 🏃 🏃</span><br><span class="line">pip install git+https://github.com/NVlabs/tiny-cuda-nn/#subdirectory=bindings/torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">WARNING: Using a slow implementation <span class="keyword">for</span> the MLPWithHashEncoding module. </span><br><span class="line">🏃 🏃 Install tcnn <span class="keyword">for</span> speedups 🏃 🏃</span><br><span class="line">pip install git+https://github.com/NVlabs/tiny-cuda-nn/#subdirectory=bindings/torch</span><br><span class="line"></span><br><span class="line">WARNING: Using a slow implementation <span class="keyword">for</span> the MLP module. </span><br><span class="line">🏃 🏃 Install tcnn <span class="keyword">for</span> speedups 🏃 🏃</span><br><span class="line">pip install git+https://github.com/NVlabs/tiny-cuda-nn/#subdirectory=bindings/torch</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">WARNING: Using a slow implementation <span class="keyword">for</span> the HashEncoding module. </span><br><span class="line">🏃 🏃 Install tcnn <span class="keyword">for</span> speedups 🏃 🏃</span><br><span class="line">pip install git+https://github.com/NVlabs/tiny-cuda-nn/#subdirectory=bindings/torch</span><br></pre></td></tr></table></figure></div>
<p>提示你可以用tcnn进行加速。根据它的提示输入指令：</p>
<div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">pip install git+https://github.com/NVlabs/tiny-cuda-nn/#subdirectory=bindings/torch</span><br></pre></td></tr></table></figure></div>
<p>不出意外的话就要出意外了：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/a212c4626ca45c549cddfa3d72efbbc3.png"
                      alt="error" style="zoom: 80%;" 
                ></p>
<p>先是查看了下文档，说是要求 g++ &lt; 11 ，于是安装了g++-9：</p>
<div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">sudo</span> apt install g++-9</span><br></pre></td></tr></table></figure></div>
<p>然后切换版本：</p>
<div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">sudo</span> update-alternatives --install /usr/bin/g++ g++ /usr/bin/g++-9 20</span><br></pre></td></tr></table></figure></div>
<p>如果有多个版本好像还得执行以下指令切换： <div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">sudo</span> update-alternatives --config g++</span><br></pre></td></tr></table></figure></div></p>
<p>发现还是不行，看报错里有这样一句：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/57fec14ddd5574caba775cff7b69445e.png"
                      alt="error2" style="zoom:80%;" 
                ></p>
<p>说明问题出在<code>lcuda</code>，g++找不到<code>lcuda</code>。因为博主使用的WSL，<code>cuda</code>库存放在<code>/usr/lib/wsl/lib</code>中，将它复制出来即可：</p>
<div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">sudo</span> <span class="built_in">cp</span> /usr/lib/wsl/lib/* /usr/lib</span><br></pre></td></tr></table></figure></div>
<p>然后再次执行安装就成功了。</p>
<p>值得注意的是，在安装<code>tcnn</code>之前，博主用<code>nerfacto</code>训练30000个step用了两小时，而安装之后仅需20分钟，这个提升还是蛮可观的。</p>
<h2 id="报错及解决方案">报错及解决方案</h2>
<h3 id="使用splatfacto训练报错">使用splatfacto训练报错</h3>
<h4 id="no-cuda-toolkit-found.">1. No CUDA toolkit found.</h4>
<p>在使用splatfacto进行训练时报错：
<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/16ca70d9990e612c0f7fce1c4513128b.png"
                      alt="error3" style="zoom:80%;" 
                ></p>
<p>显示CUDA Tookit找不到，然而我的用户目录里是有的。</p>
<p>在github的issue里找到了解决方案：<a class="link" 
 href="https://github.com/nerfstudio-project/gsplat/issues/249" >No CUDA
toolkit found. gsplat will be disabled. · Issue #249 ·
nerfstudio-project/gsplat<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>
<p>即，将path添加进去：</p>
<div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> PATH=/usr/local/cuda-12.6/bin<span class="variable">$&#123;PATH:+:<span class="variable">$&#123;PATH&#125;</span>&#125;</span></span><br></pre></td></tr></table></figure></div>
<p>可以将这句添加到<code>~/.bashrc</code>里，每次打开terminal就不用再输入一遍了。</p>
<h4 id="ninja-build-stopped-subcommand-failed.">2. ninja: build stopped:
subcommand failed.</h4>
<p>解决上个问题后结果还是报错：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/19d2701071cf3a88991cac45b0e1de85.png"
                      alt="error4" style="zoom:80%;" 
                ></p>
<p>查阅发现是内存不够，进程直接被kill了。自己的WSL虚拟机内存太少了。尝试租服务器，解决。</p>
<h3
id="使用nerfbusters训练报错未完全解决">使用nerfbusters训练报错（未完全解决）</h3>
<h4
id="modulenotfounderror-no-module-named-nerfstudio.fields.visibility_field">1.
ModuleNotFoundError: No module named
'nerfstudio.fields.visibility_field'</h4>
<p>在使用<code>nerfbusters</code>方法时，根据<a class="link" 
 href="https://docs.nerf.studio/nerfology/methods/nerfbusters.html" >文档<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>中的教程安装<code>nerfbuster</code>之后，简单的使用<code>--help</code>也会出现如下的报错：</p>
<div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">ModuleNotFoundError: No module named <span class="string">&#x27;nerfstudio.fields.visibility_field&#x27;</span></span><br></pre></td></tr></table></figure></div>
<p>切换其他基于<code>NeRF</code>的方法，有的依然会出现这个报错。</p>
<p>然后在<code>issue</code>中找到了相似的情况：</p>
<p><a class="link"   href="https://github.com/ethanweber/nerfbusters/issues/17" >Where's
nerfstudio VisibilityFIeld come from? · Issue #17 ·
ethanweber/nerfbusters<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>
<p><a class="link" 
 href="https://github.com/nerfstudio-project/nerfstudio/issues/3185" >from
nerfstudio.fields.visibility_field import VisibilityField
ModuleNotFoundError: No module named
'nerfstudio.fields.visibility_field' · Issue #3185 ·
nerfstudio-project/nerfstudio<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>
<p><a class="link" 
 href="https://github.com/nerfstudio-project/nerfstudio/pull/2264" >Visibility
Field from Nerfbusters by ethanweber · Pull Request #2264 ·
nerfstudio-project/nerfstudio<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>
<p>其中提到，他们当前使用的branch是<code>nerfbusters-changes</code>，并没有计划把他合并到<code>main</code>
branch。</p>
<p>所以需要克隆他们的<code>nerfbusters-changes</code> branch：</p>
<div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> -b nerfbusters-changes https://github.com/nerfstudio-project/nerfstudio.git</span><br></pre></td></tr></table></figure></div>
<p>然后在根目录执行安装：</p>
<div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">pip install -e .</span><br></pre></td></tr></table></figure></div>
<p>这样就可以了。</p>
<h4 id="numpy-has-no-attribute-bool8.-did-you-mean-bool">2. `numpy` has
no attribute `bool8`. Did you mean: `bool`?</h4>
<p>这是因为numpy在1.24更新后将<code>bool8</code>更名为了<code>bool</code>，降级numpy版本即可：</p>
<div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">pip install numpy==1.23</span><br></pre></td></tr></table></figure></div>
<h4 id="the-viewer-bridge-server-subprocess-failed.">3. The viewer
bridge server subprocess failed.</h4>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/511ceec6536981ff7b744e6a521940cd.png"
                      alt="image-20241126213428268" style="zoom:80%;" 
                ></p>
<p>切换分支后运行原有的方法都会出现如下报错。说是<code>viewer</code>的服务启动失败了，通过<code>--viewer.websocket-port</code>更改窗口依然是相同的报错，于是根据提示查看了log：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/e26e3f4efeb2a6e74a7b49493d5bf70b.png"
                      alt="image-20241126213554646" style="zoom:80%;" 
                ></p>
<p>好嘛，给我原来的module搞没了，我又回原来的分支重新<code>pip install -e .</code>，然后再运行。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/33aa4114532b180a7cb9cc5a485ea9bf.png"
                      alt="image-20241126214834818" style="zoom:80%;" 
                ></p>
<p>闹鬼了，我不玩了行吧，nerfbusters给劳资滚！😠</p>
<h3 id="使用zipnerf报错">使用zipnerf报错</h3>
<h4
id="error-failed-building-wheel-for-cuda_backend-或者-no-module-named-_cuda_backend">1.
ERROR: Failed building wheel for cuda_backend 或者 No module named
'_cuda_backend'</h4>
<p>在使用</p>
<div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">pip install git+https://github.com/SuLvXiangXin/zipnerf-pytorch#subdirectory=extensions/cuda</span><br></pre></td></tr></table></figure></div>
<p>安装依赖的时候，出现如下报错：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/dd5fcccf23bab67884491c42c534f451.png"
                      alt="image-20241126233030691" style="zoom:80%;" 
                ></p>
<p>开始没有管他，直到最后训练的时候又弹出报错：</p>
<figure>
<img 

                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/3f39f4928d4cdc95d4268681db55ca98.png"
                     
alt="image-20241126233117488" 
                >
<figcaption aria-hidden="true">image-20241126233117488</figcaption>
</figure>
<p>看样子是逃不掉了。</p>
<h4 id="assertionerror-pipeline.datamanager.dataparser...">2.
AssertionError: <code>pipeline.datamanager.dataparser</code>...</h4>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/0bbf42cce63eb2803ee4bac0f676f5a9.png"
                      alt="image-20241126233225832" style="zoom:80%;" 
                ></p>
<p>第一次出现这长串报错是因为参数没输对，第二次问gpt说是与
<code>tyro</code> 版本有关，尝试升级 <code>tyro</code>
库以及相关依赖：</p>
<div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">pip install --upgrade tyro</span><br></pre></td></tr></table></figure></div>
<p>然后看到error怂了：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/bda407fee5eb83019937880b7434e75b.png"
                      alt="image-20241126233430107" style="zoom:80%;" 
                ></p>
<p>于是又改回了推荐的版本：</p>
<div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">pip install tyro==2.13.3</span><br></pre></td></tr></table></figure></div>
<p>然后再运行就可以了。</p>
<h4
id="assertionerror-colmap-path-dataprocessed_trucksparse0-does-not-exist.">3.
AssertionError: Colmap path data/processed_truck/sparse/0 does not
exist.</h4>
<p>这个原因是<code>zipnerf</code>用的数据集格式和<code>nerfstudio</code>好像不完全一致，我用了下<code>tandt</code>的数据集发现可行，但是它会对图像先进行一次下采样。</p>
<div class="code-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">(nerf2mesh) root@I1dc2923c3f00801cdf:~/3D-Reconstruction/nerf2mesh# python main.py nerfstudio/poster/ --workspace trial_syn_poster/ -O --bound 1 --scale 0.8 --dt_gamma 0 --stage 0 --lambda_tv 1e-8</span><br><span class="line">Warning:</span><br><span class="line">Unable to load the following plugins:</span><br><span class="line"></span><br><span class="line">        libio_e57.so: libio_e57.so does not seem to be a Qt Plugin.</span><br><span class="line"></span><br><span class="line">Cannot load library /usr/local/miniconda3/envs/nerf2mesh/lib/python3.10/site-packages/pymeshlab/lib/plugins/libio_e57.so: (/usr/lib/x86_64-linux-gnu/libp11-kit.so.0: undefined symbol: ffi_type_pointer, version LIBFFI_BASE_7.0)</span><br><span class="line"></span><br><span class="line">Loading train data: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 225/225 [00:08&lt;00:00, 25.09it/s]</span><br><span class="line">[INFO] max_epoch 134, eval every 26, save every 2.</span><br><span class="line">[INFO] Trainer: ngp_stage0 | 2024-11-26_15-12-23 | cuda | fp16 | trial_syn_poster/</span><br><span class="line">[INFO] #parameters: 18367240</span><br><span class="line">Namespace(path=&#x27;nerfstudio/poster/&#x27;, O=True, workspace=&#x27;trial_syn_poster/&#x27;, seed=0, stage=0, ckpt=&#x27;latest&#x27;, fp16=True, sdf=False, tcnn=False, progressive_level=False, test=False, test_no_video=False, test_no_mesh=False, camera_traj=&#x27;&#x27;, data_format=&#x27;nerf&#x27;, train_split=&#x27;train&#x27;, preload=True, random_image_batch=True, </span><br><span class="line">downscale=1, bound=1.0, scale=0.8, offset=[0, 0, 0], mesh=&#x27;&#x27;, enable_cam_near_far=False, enable_cam_center=False, min_near=0.05, enable_sparse_depth=False, enable_dense_depth=False, iters=30000, lr=0.01, lr_vert=0.0001, pos_gradient_boost=1, cuda_ray=True, max_steps=1024, update_extra_interval=16, </span><br><span class="line">max_ray_batch=4096, grid_size=128, mark_untrained=True, dt_gamma=0.0, density_thresh=10, diffuse_step=1000, diffuse_only=False, background=&#x27;random&#x27;, enable_offset_nerf_grad=False, n_eval=5, n_ckpt=50, num_rays=4096, adaptive_num_rays=True, num_points=262144, lambda_density=0, lambda_entropy=0, lambda_tv=1e-08, </span><br><span class="line">lambda_depth=0.1, lambda_specular=1e-05, lambda_eikonal=0.1, lambda_rgb=1, lambda_mask=0.1, wo_smooth=False, lambda_lpips=0, lambda_offsets=0.1, lambda_lap=0.001, lambda_normal=0, lambda_edgelen=0, contract=False, patch_size=1, trainable_density_grid=False, color_space=&#x27;srgb&#x27;, ind_dim=0, ind_num=500, </span><br><span class="line">mcubes_reso=512, env_reso=256, decimate_target=300000.0, mesh_visibility_culling=True, visibility_mask_dilation=5, clean_min_f=8, clean_min_d=5, ssaa=2, texture_size=4096, refine=True, refine_steps_ratio=[0.1, 0.2, 0.3, 0.4, 0.5, 0.7], refine_size=0.01, refine_decimate_ratio=0.1, refine_remesh_size=0.02, </span><br><span class="line">vis_pose=False, gui=False, W=1000, H=1000, radius=5, fovy=50, max_spp=1, refine_steps=[3000, 6000, 9000, 12000, 15000, 21000])</span><br><span class="line">NeRFNetwork(</span><br><span class="line">  (encoder): GridEncoder: input_dim=3 num_levels=16 level_dim=1 resolution=16 -&gt; 2048 per_level_scale=1.3819 params=(6119864, 1) gridtype=hash align_corners=False interpolation=linear</span><br><span class="line">  (sigma_net): MLP(</span><br><span class="line">    (net): ModuleList(</span><br><span class="line">      (0): Linear(in_features=19, out_features=32, bias=False)</span><br><span class="line">      (1): Linear(in_features=32, out_features=1, bias=False)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (encoder_color): GridEncoder: input_dim=3 num_levels=16 level_dim=2 resolution=16 -&gt; 2048 per_level_scale=1.3819 params=(6119864, 2) gridtype=hash align_corners=False interpolation=linear</span><br><span class="line">  (color_net): MLP(</span><br><span class="line">    (net): ModuleList(</span><br><span class="line">      (0): Linear(in_features=35, out_features=64, bias=False)</span><br><span class="line">      (1): Linear(in_features=64, out_features=64, bias=False)</span><br><span class="line">      (2): Linear(in_features=64, out_features=6, bias=False)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">  (specular_net): MLP(</span><br><span class="line">    (net): ModuleList(</span><br><span class="line">      (0): Linear(in_features=6, out_features=32, bias=False)</span><br><span class="line">      (1): Linear(in_features=32, out_features=3, bias=False)</span><br><span class="line">    )</span><br><span class="line">  )</span><br><span class="line">)</span><br><span class="line">[INFO] Loading latest checkpoint ...</span><br><span class="line">[WARN] No checkpoint found, abort loading latest model.</span><br><span class="line">Loading val data: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00&lt;00:00, 21.10it/s]</span><br><span class="line">[mark untrained grid] 9364 from 2097152</span><br><span class="line">==&gt; Start Training Epoch 1, lr=0.000100 ...</span><br><span class="line">... ...</span><br><span class="line">loss=0.091703 (0.104557) lr=0.004555: : 100% 225/225 [00:03&lt;00:00, 69.94it/s]</span><br><span class="line">==&gt; Finished Epoch 133, loss = 0.078408.</span><br><span class="line">==&gt; Start Training Epoch 134, lr=0.001006 ...</span><br><span class="line">loss=0.075468 (0.078282) lr=0.000988: : 100% 225/225 [00:04&lt;00:00, 52.46it/s]</span><br><span class="line">==&gt; Finished Epoch 134, loss = 0.078282.</span><br><span class="line">[INFO] training takes 10.078925 minutes.</span><br><span class="line">Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]</span><br><span class="line">/usr/local/miniconda3/envs/nerf2mesh/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter &#x27;pretrained&#x27; is deprecated since 0.13 and may be removed in the future, please use &#x27;weights&#x27; instead.</span><br><span class="line">  warnings.warn(</span><br><span class="line">/usr/local/miniconda3/envs/nerf2mesh/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for &#x27;weights&#x27; are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.</span><br><span class="line">  warnings.warn(msg)</span><br><span class="line">Downloading: &quot;https://download.pytorch.org/models/vgg16-397923af.pth&quot; to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth</span><br><span class="line">100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 528M/528M [35:11&lt;00:00, 262kB/s]</span><br><span class="line">Loading model from: /usr/local/miniconda3/envs/nerf2mesh/lib/python3.10/site-packages/lpips/weights/v0.1/vgg.pth</span><br><span class="line">++&gt; Evaluate at epoch 134 ...</span><br><span class="line">loss=0.213495 (0.213495): : 100% 1/1 [00:00&lt;00:00,  1.07it/s]</span><br><span class="line">PSNR = 6.706122</span><br><span class="line">LPIPS (vgg) = 0.767062</span><br><span class="line">++&gt; Evaluate epoch 134 Finished, loss = 0.213495</span><br><span class="line">==&gt; Start Test, save results to trial_syn_poster/results</span><br><span class="line">100% 11/11 [00:03&lt;00:00,  3.40it/s]Traceback (most recent call last):</span><br><span class="line">  File &quot;/root/3D-Reconstruction/nerf2mesh/main.py&quot;, line 263, in &lt;module&gt;</span><br><span class="line">    trainer.test(test_loader, write_video=True) # test and save video</span><br><span class="line">  File &quot;/root/3D-Reconstruction/nerf2mesh/nerf/utils.py&quot;, line 1005, in test</span><br><span class="line">    imageio.mimwrite(os.path.join(save_path, f&#x27;&#123;name&#125;_rgb.mp4&#x27;), all_preds, fps=24, quality=8, macro_block_size=1)</span><br><span class="line">  File &quot;/usr/local/miniconda3/envs/nerf2mesh/lib/python3.10/site-packages/imageio/v2.py&quot;, line 494, in mimwrite</span><br><span class="line">    with imopen(uri, &quot;wI&quot;, **imopen_args) as file:</span><br><span class="line">  File &quot;/usr/local/miniconda3/envs/nerf2mesh/lib/python3.10/site-packages/imageio/core/imopen.py&quot;, line 281, in imopen</span><br><span class="line">    raise err_type(err_msg)</span><br><span class="line">ValueError: Could not find a backend to open `trial_syn_poster/results/ngp_stage0_ep0134_rgb.mp4`` with iomode `wI`.</span><br><span class="line">Based on the extension, the following plugins might add capable backends:</span><br><span class="line">  FFMPEG:  pip install imageio[ffmpeg]</span><br><span class="line">  pyav:  pip install imageio[pyav]</span><br><span class="line">100% 11/11 [00:04&lt;00:00,  2.67it/s]</span><br></pre></td></tr></table></figure></div>
<h2 id="运行结果">运行结果</h2>
<h3
id="nerfacto和splatfacto渲染效果对比">nerfacto和splatfacto渲染效果对比</h3>
<p>由于背景场景太过杂乱，在导出的时候选择了crop一下，只保留了主体。</p>
<p>对于基于<code>NeRF</code>的方法<code>nerfacto</code>，可以选择导出点云或者网格。导出的网格在<code>meshlab</code>进行可视化，效果如下：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/3352878e1ae4701c1140250512516ff5.png"
                      alt="nerfacto" style="zoom: 67%;" 
                ></p>
<p>可以看到，<code>poster</code>的内容是比较清晰地还原出来了，然而椅子的形状结构却有些损坏，尤其是越靠近中心缺损越严重。</p>
<p>而对于基于<code>3DGS</code>的方法<code>splatfacto</code>，它只能选择导出gaussian
splat。为了对其进行可视化，选择使用了 <a class="link" 
 href="https://github.com/playcanvas/supersplat" >supersplat<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>
这个工具：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/fe681800c80665393e977b430120f321.png"
                      alt="1" style="zoom:67%;" 
                ></p>
<p>可以看到，除了中心的主体外，为了渲染出背景，在即便是很远的地方也生成了很多个gaussian
splat。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/684268fc7c4fbd9c6ccc174ea9ae2481.png"
                      alt="image-20241119133706308" style="zoom:67%;" 
                ></p>
<p>将摄像头拉近到主体，发现这个重构效果还是蛮好的。然而如何将gaussian
splat转化为网格形式，这是后续工作的重点。</p>
<h3 id="mesh导出">Mesh导出</h3>
<p>在<code>NeRF Studio</code>中可以对训练结果进行可视化的导出。支持<code>3D gaussian</code>,
<code>point cloud</code>和<code>mesh</code>
，但是对于不同的方法，导出的格式不同。如基于<code>NeRF</code>的方法只能导出点云和网格，而基于<code>3DGS</code>的方法只能导出三维高斯。</p>
<p>训练代码示例：</p>
<div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">ns-train nerfacto --data data/processed_truck/ --output-dir outputs/truck_100000 --max-num-iterations 100000</span><br></pre></td></tr></table></figure></div>
<p>可以先对训练结果进行可视化：</p>
<div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">ns-viewer --load-config path/to/your/trainresult/config.yml</span><br></pre></td></tr></table></figure></div>
<p>然后在网页的<code>export</code>栏可以选择导出参数，并复制导出指令，以<code>possion</code>的导出为例：</p>
<div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">ns-export poisson --load-config outputs/truck_100000/processed_truck/nerfacto/2024-11-26_142623/config.yml --output-dir exports/mesh/truck_100000iter_50000face --target-num-faces 50000 --num-pixels-per-side 2048 --num-points 1000000 --remove-outliers True </span><br><span class="line">--normal-method open3d --obb_center 0.0000000000 0.0000000000 0.0000000000 --obb_rotation 0.0000000000 0.0000000000 0.0000000000 --obb_scale 1.0000000000 1.0000000000 1.0000000000</span><br></pre></td></tr></table></figure></div>
<p>还可以选择其他的导出方式，如<code>tsdf</code>：</p>
<div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">ns-export tsdf --load-config outputs/truck_100000/processed_truck/nerfacto/2024-11-26_142623/config.yml --output-dir exports/mesh/truck_100000iter_50000face_tsdf --target-num-faces 50000 --num-pixels-per-side 2048</span><br></pre></td></tr></table></figure></div>
<p>或者<code>marching-cubes</code>：</p>
<div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">ns-export marching-cubes --load-config outputs/truck_100000/processed_truck/nerfacto/2024-11-26_142623/config.yml --output-dir exports/mesh/truck_100000iter_50000face_tsdf --target-num-faces 50000 --num-pixels-per-side 2048</span><br></pre></td></tr></table></figure></div>
<p>（好像<code>nerfacto</code>的结果不能用<code>marching-cubes</code>）</p>
<p>下面直观展示下导出<code>mesh</code>的效果（前四种是<code>nerfacto</code>，最后一个是用<code>2DGS</code>方法生成的结果）：</p>
<table>
<colgroup>
<col style="width: 37%" />
<col style="width: 62%" />
</colgroup>
<thead>
<tr>
<th style="text-align: center;">possion (30k iters, 50k faces)</th>
<th><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/1c1ac2e6320eaf5e8bcd849e54715188.gif"
                      alt="2024-11-26-17-05-29" style="zoom: 80%;" 
                ></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"><strong>possion (100k iters, 50k
faces)</strong></td>
<td><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/5392674dd8736a531079d3f8643d6d7c.gif"
                      alt="2024-11-26-17-06-26" style="zoom: 80%;" 
                ></td>
</tr>
<tr>
<td style="text-align: center;"><strong>possion (100k iters, 100k
faces)</strong></td>
<td><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/299d0d4cf36fa66f3195efdd15ec7bac.gif"
                      alt="2024-11-26-17-08-08" style="zoom:80%;" 
                ></td>
</tr>
<tr>
<td style="text-align: center;"><strong>tsdf (100k iters, 50k
faces)</strong></td>
<td><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/65ea32e143162450ebfa80c39fc288bc.gif"
                      alt="2024-11-26-16-52-37" style="zoom: 80%;" 
                ></td>
</tr>
<tr>
<td style="text-align: center;"><strong>2DGS (2m faces)</strong></td>
<td><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/34ed3d848689502c6897a0aee256bace.gif"
                      alt="2024-11-26-16-52-37" style="zoom: 80%;" 
                ></td>
</tr>
</tbody>
</table>
<p>可以看到，<code>tsdf</code>提取的网格质量过低，而<code>possion</code>提取的网格相较而言则更精确。总体而言，对于<code>nerfacto</code>方法来说，<code>iteration</code>的提升貌似没有对最终的网格产生比较大的改善，30k的迭代次数已经足够，而<code>face</code>的增加其实也显得不是很必要，50k的面数已经足够表达一个复杂的结构体了。对于<code>2DGS</code>，由于它没有整合到<code>NeRF Studio</code>里，它的重建没有固定面数，最终生成了2m的面，虽然重建效果好很多，但是对计算负载的压力更大。</p>
]]></content>
      <categories>
        <category>三维重建</category>
      </categories>
      <tags>
        <tag>NeRF</tag>
        <tag>3DGS</tag>
      </tags>
  </entry>
  <entry>
    <title>主流三维重建方法对比</title>
    <url>/2024/11/18/9-Reconstruction/</url>
    <content><![CDATA[<h2 id="meshanything">MeshAnything</h2>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/1630f28c1eaf60eb65bdd5e72f47430d.gif"
                      alt="demo_video" style="zoom: 67%;" 
                ></p>
<h3 id="问题背景">问题背景</h3>
<p>当前网格提取方法生成的网格明显不如艺术家创建的网格 (AM,
Artist-Created
Meshes)，即由人类艺术家创建的网格。具体来说，当前的网格提取方法依赖于密集的面并忽略几何特征，导致效率低下、后处理复杂且表示质量较低等问题。为了解决这些问题，论文引入了
<a class="link"   href="https://github.com/buaacyw/MeshAnything" >MeshAnything<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>
，该模型将网格提取视为生成问题，生成与指定形状对齐的 AM。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/5febb2fa38db1681a29897d2bf6f4f5e.png"
                      alt="Refer to caption" style="zoom:50%;" 
                ></p>
<p>上图是对真实形状使用<code>Marching Cubes</code>和<code>MeshAnything</code>，然后对不同<code>voxel size</code>的<code>Marching Cubes</code>进行<code>remesh</code>的结果。现有方法以重构方式提取网格，忽略了对象的几何特征并产生拓扑较差的密集网格。这些方法从根本上无法捕捉锐利的边缘和平坦的表面，如放大图所示。</p>
<h3 id="核心方法">核心方法</h3>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/2ccc528c2caa28dece7df53cd9041b5e.png"
                      alt="image-20241119142104541" style="zoom: 50%;" 
                ></p>
<p><strong>重新定义问题</strong>：</p>
<ul>
<li>将网格提取看作一个生成任务，而非重建任务。</li>
<li>目标是生成与给定形状对齐的高效拓扑网格（AMs），使其更接近人工设计。</li>
</ul>
<p><strong>模型架构</strong>：</p>
<ul>
<li>使用 <strong>VQ-VAE</strong> 学习网格的“词汇”（网格编码）。</li>
<li>基于“词汇”训练一个<strong>仅解码自回归Transformer</strong>，以形状条件为输入，生成网格。</li>
<li>增强了
<strong>VQ-VAE解码器</strong>，使其抗噪，能够处理低质量的令牌序列，并利用形状信息辅助解码。</li>
</ul>
<p><strong>形状条件的设计</strong>：</p>
<ul>
<li>选择点云作为输入形状条件，因其具有连续表示性且易于编码。</li>
<li>在训练时通过对真实网格进行降质（如使用Marching
Cubes生成粗网格）来缩小训练与推理的域差距。</li>
</ul>
<p><strong>优化训练</strong>：</p>
<ul>
<li>在VQ-VAE训练完成后，额外细调解码器，模拟推理中可能生成的低质量令牌序列，并提高对噪声的鲁棒性。</li>
</ul>
<h3 id="实验结果">实验结果</h3>
<ul>
<li><strong>生成效率</strong>： MeshAnything
生成的网格面数比传统方法减少数百倍，同时保留较高的形状对齐质量。</li>
<li><strong>拓扑质量</strong>：
在用户研究中，MeshAnything的形状和拓扑质量显著优于现有方法，表明其生成的网格更接近艺术家手工制作的标准。</li>
<li><strong>泛化能力</strong>：
能处理来自各种3D表示（如点云、体素、NeRF等）的输入，支持广泛的3D资产生产管线。</li>
</ul>
<h3 id="存在问题">存在问题</h3>
<p>第一眼见这个工作的时候，有被震撼到。以为这个工作将多种三维重建方式整合起来了。从它的演示中，看见它可以从三维高斯、图像、文本直接生成几何形状完整但面数较精简的网格。然而自己部署运行之后发现好像并不是这样。</p>
<p>在查阅代码仓库的<code>readme</code>时，发现所给的示例代码全是从点云或者网格进行转化的代码。正疑惑如何将其他表达方式进行转化，于是看了下<code>issues</code>：</p>
<ol type="1">
<li><p>关于3D Gaussian转为mesh：</p>
<p><a class="link"   href="https://github.com/buaacyw/MeshAnything/issues/29" >About 3D
Gaussians · Issue #29 · buaacyw/MeshAnything<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>
<blockquote>
<p>We suggest converting the 3D Gaussians into a mesh before inputting
them into our method.</p>
</blockquote>
<p>作者直接建议将3D Gaussian转为mesh再作为该方法的输入。（迷惑）</p>
<p><a class="link"   href="https://github.com/buaacyw/MeshAnything/issues/6" >How to
convert 3D gaussian splatting to mesh? · Issue #6 ·
buaacyw/MeshAnything<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>
<blockquote>
<p>Hi, I use <a class="link"   href="https://github.com/Anttwo/SuGaR" >SUGAR<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a> to get
high quality mesh from GS and then sample point cloud on the 3D GS.
Other methods like <a class="link" 
 href="https://github.com/hbb1/2d-gaussian-splatting" >2DGS<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a> should
also work well. I suggest you run these methods directly in their repos
and use the results as inputs for our method.</p>
</blockquote>
<p>另一个<code>issue</code>里，作者提到了两个方法来进行这种转化：SuGaR和2DGS。</p></li>
<li><p>关于大规模点云转mesh：</p>
<p><a class="link"   href="https://github.com/buaacyw/MeshAnything/issues/9" >large
scale pointCloud · Issue #9 · buaacyw/MeshAnything<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>
<p>MeshAnything目前限制在800个face，对复杂mesh或大规模点云可能无法比较好地转化。</p></li>
<li><p>关于image和text转mesh：</p>
<p><a class="link"   href="https://github.com/buaacyw/MeshAnything/issues/4" >How to do
text and images? · Issue #4 · buaacyw/MeshAnything<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a></p>
<blockquote>
<p>The image/text to mesh is achieved by combining with 3D generation
methods. We first obtain dense meshes from 3D generation methods and use
them as input to our methods. Note that the shape quality of dense
meshes should be high enough. Thus, feed-forward 3D generation methods
may often produce bad results due to insufficient shape quality. We
suggest using results from SDS-based pipelines (like DreamCraft3D) as
the input of MeshAnything as they produce better shape quality.</p>
</blockquote>
<p>意思就是，通过其他方法生成<code>dense mesh</code>，然后再作为该方法的输入呗？</p></li>
</ol>
<p>大概总结一下就是，这个方法只是将已有的<code>Mesh</code>转化为<code>Artist-Created Meshes</code>而已，包括之前提到的3DGS、image、text转mesh，根本就是通过别的方法实现的。它是将精度高的密mesh在保留大部分几何特征的情况下进行的“简化”和“优化”，让面片数更精简且拓扑结构更符合人类直觉。</p>
<p>（吐槽一句，我感觉这个标题还有最开始的演示，有种标题党的意味。还以为真的是mesh
anything）</p>
<h2 id="sugar">SuGaR</h2>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/f5a25af6a79c1435d4eac688c356059b.gif"
                      alt="kitchen_hybrid.gif" style="zoom:67%;" 
                ></p>
<h3 id="问题背景-1">问题背景</h3>
<p>3D Gaussian Splatting 是一种高效的 3D
场景渲染方法，但其优化后生成的大量无结构的 3D Gaussians
不便于生成可编辑的网格（Mesh）。</p>
<p>当前使用 Neural SDF
提取网格的方法计算量大，需多GPU训练，且耗时较长（通常需24小时以上）。</p>
<h3 id="核心方法-1">核心方法</h3>
<p><strong>表面对齐正则化</strong>：</p>
<ul>
<li>引入一个正则化项，强制优化后的 3D Gaussians
分布于场景表面，从而更好地捕获场景几何信息。</li>
<li>通过优化 Signed Distance Function (SDF)，使 Gaussians
平坦化，并沿着表面分布。</li>
</ul>
<p><strong>高效网格提取</strong>：</p>
<ul>
<li>基于优化后的 Gaussians，采样水平集上的点，使用 Poisson
重建算法生成三角形网格。相比于 Marching Cubes
算法，该方法对稀疏数据更鲁棒，且计算更高效。</li>
</ul>
<p><strong>绑定 Gaussians 到网格</strong>：</p>
<ul>
<li>将优化后的 Gaussians 与生成的网格表面绑定，进一步优化 Gaussians
和网格，通过 Gaussian Splatting 渲染，提升渲染质量。</li>
<li>这种绑定使得可以通过编辑网格来实现对场景的修改。</li>
</ul>
<h3 id="实验结果-1">实验结果</h3>
<ul>
<li><strong>速度</strong>：SuGaR 方法在单 GPU
上仅需几分钟即可提取出高质量网格，远快于基于 SDF 的方法。</li>
<li><strong>质量</strong>：在多个数据集上的定量评估显示，SuGaR
的渲染质量（PSNR、SSIM、LPIPS）优于其他基于网格的方法，并与仅关注渲染质量的顶尖方法接近。</li>
<li><strong>可编辑性</strong>：生成的网格与绑定的 Gaussians
可以直接用于动画、重光照、雕刻等操作，提供了更高的灵活性。</li>
</ul>
<h3 id="踩坑记录">踩坑记录</h3>
<ol type="1">
<li><p><strong>environment.yml 创建环境
fail</strong>：创建空环境一步一步安装依赖。</p></li>
<li><p><strong>pytorch找不到包</strong>：使用国内镜像（需要注意是使用<code>-f</code>而不是<code>--index-url</code>）：</p>
<div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">pip install torch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 -f https://mirrors.aliyun.com/pytorch-wheels/cu118</span><br></pre></td></tr></table></figure></div></li>
<li><p><strong>pytorch3d找不到包</strong>：去<a class="link" 
 href="https://anaconda.org/pytorch3d/pytorch3d/files?version=0.7.4" >Anaconda官网<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>找到对应版本的安装包复制链接再install：</p>
<div class="code-container" data-rel="Bash"><figure class="iseeu highlight bash"><table><tr><td class="code"><pre><span class="line">conda install https://anaconda.org/pytorch3d/pytorch3d/0.7.4/download/linux-64/pytorch3d-0.7.4-py39_cu118_pyt201.tar.bz2</span><br></pre></td></tr></table></figure></div></li>
<li><p><strong>其他 conda 包找不到</strong>：直接pip
insatll。（换了国内源都没解决掉，只能出此下策）</p></li>
<li><p><strong>CUDA-capable device(s) is/are busy or
unavailable</strong>：不会解决了，换回AutoDL了。在AutoDL配环境没遇到这么多问题。</p></li>
<li><p><strong>卡在 'computing mesh...' </strong>: 在 <a class="link" 
 href="https://github.com/Anttwo/SuGaR/issues/219" >issue<i class="fa-solid fa-arrow-up-right ml-[0.2em] font-light align-text-top text-[0.7em] link-icon"></i></a>
看到了一模一样的情况，目前还没解决这个问题。</p></li>
</ol>
<h2 id="dgs">2DGS</h2>
<h3 id="问题背景-2">问题背景</h3>
<p>在计算机图形学和视觉领域，<strong>视角一致的几何重建</strong>与<strong>真实感的新视角合成（NVS）</strong>一直是重要的研究目标。近年来，基于
<strong>3D Gaussian Splatting (3DGS)</strong>
的显式方法因其实时渲染能力而受到关注。然而，3DGS
在以下方面存在显著问题：</p>
<ol type="1">
<li><strong>表面几何不精确</strong>：3D高斯的体积表示与物体的薄表面特性冲突，导致表面重建质量较低。</li>
<li><strong>多视角不一致性</strong>：3DGS
在不同视角下计算交点时，由于投影平面变化，产生深度和法线的不一致性。</li>
</ol>
<p>为了解决这些问题，本文提出了一种新的显式建模方法——<strong>2D Gaussian
Splatting
(2DGS)</strong>，以提高几何重建的精度和视角一致性，同时保留快速渲染能力。</p>
<h3 id="核心方法-2">核心方法</h3>
<ol type="1">
<li><strong>方法概述</strong></li>
</ol>
<ul>
<li>通过二维高斯平面（即二维椭圆盘）来替代三维高斯球体，将场景建模为一组二维高斯分布的集合。</li>
<li>这些二维高斯盘通过显式的光线与平面交点计算，实现几何表面和光照的高效表示。</li>
<li>结合梯度优化，从稀疏点云和多视角图像中同时优化外观与几何。</li>
</ul>
<ol start="2" type="1">
<li><strong>关键技术</strong></li>
</ol>
<ul>
<li><p><strong>透视准确的二维高斯渲染</strong>：利用光线与二维高斯交点的显式计算，避免传统近似方法带来的透视误差。</p></li>
<li><p><strong>正则化损失</strong>：</p>
<ul>
<li><strong>深度失真损失</strong>：通过约束二维高斯沿光线分布的紧密性，确保几何表面稳定。</li>
<li><strong>法线一致性损失</strong>：约束二维高斯表面的法线方向与深度图梯度对齐，保证表面平滑性。</li>
</ul></li>
<li><p><strong>高效实现</strong>：基于CUDA的自定义内核加速训练和实时渲染。</p></li>
</ul>
<ol start="3" type="1">
<li><strong>优化流程</strong></li>
</ol>
<ul>
<li>初始化稀疏点云并结合多视角RGB图像，优化二维高斯参数（位置、缩放、方向）。</li>
<li>使用正则化损失减少噪声，增强几何表面和视角一致性。</li>
</ul>
<h3 id="实验结果-2">实验结果</h3>
<ol type="1">
<li><strong>实验设置</strong></li>
</ol>
<ul>
<li><strong>数据集</strong>：DTU、Tanks and Temples 和
Mip-NeRF360，涵盖不同场景和分辨率。</li>
<li><strong>比较方法</strong>：隐式表征方法（如NeRF、NeuS）与显式方法（如3DGS、SuGaR）。</li>
</ul>
<ol start="2" type="1">
<li><strong>主要实验结果</strong></li>
</ol>
<ul>
<li>几何重建：
<ul>
<li>在DTU数据集上，Chamfer距离优于现有方法，特别是在细节捕捉和噪声消除方面。</li>
<li>比隐式方法快100倍的训练速度，比其他显式方法（如SuGaR）快3倍以上。</li>
</ul></li>
<li>新视角合成：
<ul>
<li>在Mip-NeRF360数据集上，新视角渲染质量接近3DGS，且同时提供了更高的几何精度。</li>
</ul></li>
<li>消融实验：
<ul>
<li>验证了正则化项的有效性：缺失法线一致性会导致表面方向噪声，缺失深度失真损失会导致表面模糊。</li>
</ul></li>
</ul>
<ol start="3" type="1">
<li><strong>定量与定性表现</strong></li>
</ol>
<ul>
<li><strong>定量结果</strong>：在DTU数据集上，2DGS的Chamfer距离和PSNR均达到当前最优，同时显著减少了训练时间和模型存储需求。</li>
<li><strong>定性结果</strong>：相比3DGS和SuGaR，2DGS在复杂几何和细节捕捉上表现更优，能更好地重建噪声较少的表面。</li>
</ul>
<h3 id="重建效果">重建效果</h3>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/aed5c29dccb760e3d3e56912012efcf3.png"
                      alt="image-20241125175349810" style="zoom:50%;" 
                ></p>
<p>上图是训练30000个iteration后生成的mesh。可以看出2DGS生成的mesh还是比较精确的。</p>
<h2 id="gof">GOF</h2>
<p>未完待续...</p>
]]></content>
      <categories>
        <category>三维重建</category>
      </categories>
      <tags>
        <tag>NeRF</tag>
        <tag>3DGS</tag>
        <tag>2DGS</tag>
      </tags>
  </entry>
</search>
