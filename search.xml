<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Mujoco - CoACD简略教程</title>
    <url>/2024/09/29/CoACD_notes/</url>
    <content><![CDATA[<p>CoACD是一个凸分解工具，可以将凹模型粗略粗分解为凸几何体的并集。</p>
<h2 id="安装教程"><a href="#安装教程" class="headerlink" title="安装教程"></a>安装教程</h2><h3 id="1-克隆代码："><a href="#1-克隆代码：" class="headerlink" title="(1) 克隆代码："></a>(1) 克隆代码：</h3><div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">git clone --recurse-submodules https://github.com/SarahWeiii/CoACD.git</span><br></pre></td></tr></table></figure></div>

<h3 id="2-安装依赖："><a href="#2-安装依赖：" class="headerlink" title="(2) 安装依赖："></a>(2) 安装依赖：</h3><div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">cmake &gt;= 3.24</span><br><span class="line">g++ &gt;= 9, &lt; 12</span><br></pre></td></tr></table></figure></div>

<blockquote>
<p>在我的Ubuntu22.04中，apt里的cmake包版本是3.22，不能用。用snap成功安装3.30版本。（源码安装好像也行，不过我懒得添加系统变量，就还是用snap安装了。</p>
</blockquote>
<h3 id="3-编译"><a href="#3-编译" class="headerlink" title="(3) 编译"></a>(3) 编译</h3><div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">cd CoACD \</span><br><span class="line">&amp;&amp; mkdir build \</span><br><span class="line">&amp;&amp; cd build \</span><br></pre></td></tr></table></figure></div>

<p>​	然后编译：</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">cmake .. -DCMAKE_BUILD_TYPE=Release \</span><br><span class="line">&amp;&amp; make main -j</span><br></pre></td></tr></table></figure></div>

<blockquote>
<p>这里出了很多warning，但是好像不影响使用。</p>
</blockquote>
<h3 id="4-使用"><a href="#4-使用" class="headerlink" title="(4) 使用"></a>(4) 使用</h3><div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">./main -i PATH_OF_YOUR_MESH -o PATH_OF_OUTPUT</span><br></pre></td></tr></table></figure></div>



<h2 id="参数说明"><a href="#参数说明" class="headerlink" title="参数说明"></a>参数说明</h2><ul>
<li><strong>-nm&#x2F;–no-merge</strong> : 禁用合并后处理，默认为false。</li>
<li><strong>-c&#x2F;–max-convex-hull</strong> : 凸包上限，默认-1表示无限制。<strong>仅在启用合并时才</strong>有效。</li>
<li><strong>-ex&#x2F;–extrude</strong> : 沿着重叠面挤出相邻的凸包。</li>
<li><strong>-am&#x2F;–approximate-mode</strong> : 近似形状类型（“ch”表示凸包，“box”表示立方体）。</li>
<li><strong>–seed</strong> : 随机种子，默认是random()。</li>
</ul>
<p><strong>说明</strong>：</p>
<ol>
<li>大多数情况下，只需调整<code>threshold</code> （0.01~1）即可平衡细节程度和分解成分的数量。值越高，结果越粗，值越低，结果越细。</li>
<li>默认参数是快速版本。可以牺牲运行时间获取更多组件数量，增加<code>searching depth (-md)</code> 、 <code>searching node (-mn)</code>和<code>searching iteration (-mi)</code>可以获得更好的切割策略。</li>
</ol>
]]></content>
      <categories>
        <category>机器仿真</category>
      </categories>
      <tags>
        <tag>mujoco</tag>
        <tag>CoACD</tag>
        <tag>convex</tag>
      </tags>
  </entry>
  <entry>
    <title>GauU-Scene V2 论文解读</title>
    <url>/2024/11/05/GauU_Scene_V2/</url>
    <content><![CDATA[<p>论文：</p>
<blockquote>
<p>GauU-Scene V2: Assessing the Reliability of Image-Based Metrics with Expansive Lidar Image Dataset Using 3DGS and NeRF</p>
</blockquote>
<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/db7529b3c3e21b5e4cb4c591de2c70cb.png"
                      alt="dataset"
                ></p>
<table>
<thead>
<tr>
<th>方法</th>
<th>做法</th>
<th>缺点</th>
</tr>
</thead>
<tbody><tr>
<td>bungeenerf</td>
<td>卫星捕获图像</td>
<td>时间差异、无地面实况</td>
</tr>
<tr>
<td>KITTI</td>
<td>汽车雷达捕获点云数据</td>
<td>屋顶、高层建筑捕获存在不足</td>
</tr>
<tr>
<td>blockNeRF</td>
<td>Style Transformation解决时间差异</td>
<td>不提供公开可用的点云数据集</td>
</tr>
<tr>
<td>UrbanBIS</td>
<td>多视图相机捕获点云</td>
<td>未用高精度激光雷达</td>
</tr>
<tr>
<td>Urbanscene3D</td>
<td>无人机配合激光雷达</td>
<td>坐标差异，雷达点云和图像关系不明确</td>
</tr>
</tbody></table>
<p><strong>优势：</strong></p>
<ul>
<li>利用Zenmuse L1来获取地面真实几何，而大多数数据集（ blocknerf ）（ megaNeRF ）（ UrbanBIS ）依赖单目或多视图相机进行数据采集，这更适合新视图合成而不是场景重建。</li>
<li>提供了城市规模的信息，包括高层建筑、湖泊、山脉和屋顶，而其他数据集很少提供。</li>
<li>double-return技术，去除移动物体，确保更稳定的光照效果。</li>
<li>去除飞行路线中连续图像之间的冗余信息，图像更少，但信息量仍然具有可比性。</li>
</ul>
<h2 id="评估指标"><a href="#评估指标" class="headerlink" title="评估指标"></a>评估指标</h2><h3 id="PSNR"><a href="#PSNR" class="headerlink" title="PSNR"></a>PSNR</h3><p>PSNR(Peak signal-to-noise ratio 峰值信噪比) 用于表示信号的最大可能功率与影响其表示的保真度的破坏噪声的功率之间的比率，通常使用分贝标度表示为对数量。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/d6245af1d4a65be444fc779e62074667.png"
                      alt="MSE"
                ></p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/d0eb05ab65303b4ae37eabbfc43212b7.png"
                      alt="PSNR"
                ></p>
<h3 id="SSIM"><a href="#SSIM" class="headerlink" title="SSIM"></a>SSIM</h3><p>SSIM全称为Structural Similarity，即结构相似性。算法会提取以下三个特征。</p>
<ul>
<li><strong>亮度</strong></li>
<li><strong>对比度</strong></li>
<li><strong>结构</strong></li>
</ul>
<p>亮度的估计与平均灰度有关：<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/cfd417d0ced2bd8f64a88b7fa799f8c5.png"
                      alt="均值"
                ></p>
<p>对比度的估计则用标准差衡量：<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/ff69fd63d0e036d0866f420595dc2668.png"
                      alt="标准差"
                ></p>
<p>结构比较是通过使用一个合并公式来完成。</p>
<p>三个对比函数分别如下：</p>
<table>
<thead>
<tr>
<th></th>
<th>公式</th>
</tr>
</thead>
<tbody><tr>
<td><strong>亮度</strong></td>
<td><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/9615320354732921de0c9a7268823ca3.png"
                      alt="亮度"
                ></td>
</tr>
<tr>
<td><strong>对比度</strong></td>
<td><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/f82417d40a829e317089fc1240823192.png"
                      alt="对比度"
                ></td>
</tr>
<tr>
<td><strong>结构</strong></td>
<td><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/3765b0064f96885e010bb8eb3725455a.png"
                      alt="结构"
                >其中<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/6ba650fa5b1715045873b1422610d366.png"
                      alt="协方差"
                ></td>
</tr>
</tbody></table>
<p>结合即得到SSIM函数：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/fdd23e8d54ad6380ebc851cd06a0a12d.png"
                      alt="SSIM"
                ></p>
<h3 id="LPIPS"><a href="#LPIPS" class="headerlink" title="LPIPS"></a>LPIPS</h3><p>学习感知图像块相似度(Learned Perceptual Image Patch Similarity, LPIPS)，通过深度学习模型来评估两个图像之间的感知差异。使用预训练的深度网络（如 VGG、AlexNet）来提取图像特征，然后计算这些特征之间的距离，以评估图像之间的感知相似度。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/e63001489b8b365f3bd6a46a08578664.jpeg"
                      alt="LPIPS"
                ></p>
<p>图示是将左右两幅图像与中间图像对比的结果。可以看到，传统方法（L2&#x2F;PSNR, SSIM, FSIM）的结果与人的感知相反。而后三行通过神经网络提取特征的方法，能更符合人的感知，来评判图片的相似度。</p>
<h3 id="倒角距离"><a href="#倒角距离" class="headerlink" title="倒角距离"></a>倒角距离</h3><p>出自：</p>
<blockquote>
<p>H. Fan, S. Hao, and L. Guibas, “A point set generation network for 3D object reconstruction from a single image,” <a class="link"   href="https://so.csdn.net/so/search?q=CVPR&spm=1001.2101.3001.7020" >CVPR <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>, 2017.</p>
</blockquote>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/2c25811a255e9fb0702799148f56c8ed.png"
                      alt="Chamfer distance"
                ></p>
<p>算法：</p>
<ol>
<li>对S1中任意一点x，计算它与S2中所有点的距离，取最小距离的平方。</li>
<li>遍历S1中的点，重复1中过程，求和所有距离平方。</li>
<li>同样的步骤，对S2中所有点遍历，重复1、2过程。</li>
<li>将两个求和结果相加，作为倒角距离。</li>
</ol>
<p>倒角距离用于衡量两个点云之间的相似度。如果该距离较大，则说明两组点云区别较大；如果距离较小，则说明重建效果较好。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><h3 id="实验环境"><a href="#实验环境" class="headerlink" title="实验环境"></a>实验环境</h3><ul>
<li><strong>Vanilla Gaussian Splatting</strong>: RTX 3090 * 1</li>
<li><strong>NeRF-based models</strong>: RTX 3090 * 4</li>
</ul>
<h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/3f83245b067971c4b84de5cd9895d823.png"
                      alt="Table3"
                ></p>
<ul>
<li>3DGS和SuGaR在图像的渲染中，有更优的效果（三个参数都更优），且训练时间更短。</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/b6069787b5c845e9080f4ef1d4751090.png"
                      alt="Table4"
                ></p>
<ul>
<li>NeRF: 使用ns-export生成3D点云。</li>
<li>3DGS: 每个Gaussian Splatting的均值作为一个点，输出点云。</li>
<li>神经辐射场生成的点云，通常包含很多与场景无关的异常值。</li>
<li>3DGS也存在边缘效应，边缘会模糊。</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/b9ceed006fcc21ee2617014e59db17c1.png"
                      alt="CD"
                ></p>
<p>（图源项目网址：<a class="link"   href="https://saliteta.github.io/CUHKSZ_SMBU/" >GauU-Scene <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>）</p>
<ul>
<li>NeRFacto，虽然在图像生成得分最低，但是倒角距离最小。而Instant-NGP和SuGaR分别是倒数第一和倒数第二。这一实验结果揭示了基于图像的测量无法代表底层几何结构的基本事实。</li>
<li>SuGaR 是一种专为几何对齐设计的方法，排名却倒数。再对SuGaR进行定量分析，发现SuGaR在几何重建方面确实有更好的表现。</li>
</ul>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/3472d29a30085a95fb9679362e2c9f49.png"
                      alt="SuGaR"
                ></p>
<p>图中的蓝点在其他方法中很常见，而绿色甚至略带红色的点在其他方法中却很少见。更何况SuGaR中绿色点如此之多。 从定量的角度来看，如果我们忽略这里显示的异常值，SuGaR 的确是最好的方法。</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/0a3a7c7970e0d897124e5fa0d668a98d.png"
                      alt="Figure6"
                ></p>
<p>对高斯泼溅的alpha值进行了简单分析，其中几乎三分之二的alpha值几乎是透明的。通过删除这些值，渲染的图像变得更加清晰，几乎没有信息丢失。这些近乎透明的高斯飞溅实例漂浮在3D空间中。尽管它们在渲染图像中看不到，但它们会导致几何测量指标的退化。</p>
]]></content>
      <categories>
        <category>三维重建</category>
      </categories>
      <tags>
        <tag>NeRF</tag>
        <tag>3DGS</tag>
      </tags>
  </entry>
  <entry>
    <title>SplatSim 论文解读</title>
    <url>/2024/11/06/SplatSim/</url>
    <content><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>Sim2Real是机器人技术中的一个核心问题，涉及将模拟环境中学到的控制策略转移到现实世界环境中。目前的方法基本都依赖于深度、触觉传感或点云输入等感知方式。相比之下，RGB 图像很少用作机器人学习应用中的主要传感方式。优于 Sim2Real 传输中的其他常用方式。它们捕捉关键的视觉细节，例如颜色、纹理、照明和表面反射率等，这对于理解复杂的环境至关重要。此外，RGB 图像很容易在现实环境中使用相机获取，并且与人类感知紧密结合，使其非常适合解释动态和复杂场景中的复杂细节。</p>
<p>为什么很难将使用 RGB 信息进行模拟训练的策略部署到现实世界呢？是因为机器人在模拟器中观察到的图像分布与它在现实世界中看到的图像分布有很大不同。本文提出了一种新颖的方法来减少 RGB 图像的 Sim2Real 差距。利用 Gaussian Splatting 作为照片级真实感渲染，使用现有模拟器作为物理主干。利用 Gaussian Splatting 作为主要渲染基元，取代现有模拟器中传统的基于网格的表示，以显著提高渲染场景的照片真实感。</p>
<h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><ul>
<li><strong>关键前提</strong>：准确分割现实场景中高斯分布表示的每个刚体，并识别其相对于模拟器的相应的齐次变换。那么就可以渲染新姿势下的刚体。</li>
<li><strong>底层表示</strong>：不使用网格图元，而是使用高斯图作为底层表示。</li>
</ul>
<h3 id="A-问题描述"><a href="#A-问题描述" class="headerlink" title="A. 问题描述"></a>A. 问题描述</h3><p>$\mathcal{S}<em>{real}$表示真实场景的Gaussian Splat，$\mathcal{S}^{k}</em>{obj}$表示场景中第k个object的Gaussian Splat。我们的目标是为任何模拟器中的机器人，使用$\mathcal{S}_{real}$来生成真实的渲染$\mathcal{I}^{sim}$。然后在这样的表示下，收集专家的演示 $ε$​ 来用于训练基于RGB的策略。</p>
<h3 id="B-坐标系定义和变换"><a href="#B-坐标系定义和变换" class="headerlink" title="B. 坐标系定义和变换"></a>B. 坐标系定义和变换</h3><p>$\mathcal{F}_{real}$：真实世界坐标系 - 主要参考系。</p>
<p>$\mathcal{F}_{robot}$：真实世界机器人坐标系。</p>
<p>$\mathcal{F}_{sim}$：模拟器坐标系。</p>
<p>$\mathcal{F}<em>{robot}$和$\mathcal{F}</em>{sim}$都会与$\mathcal{F}_{real}$进行对其，以确保模拟器中机器人底座和现实世界共享相同的坐标系。</p>
<h3 id="C-机器人-Splat-模型"><a href="#C-机器人-Splat-模型" class="headerlink" title="C. 机器人 Splat 模型"></a>C. 机器人 Splat 模型</h3><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/1c7c7e02cbb7f0779e69c9b28f154dd4.png"
                      alt="coords" style="zoom:50%;" 
                >

<p>首先创建场景的高斯分布（其中机器人位于其原始位置），在静态场景中对机器人进行可视化。使用 ICP 算法手动分割机器人的点云并与标准机器人框架对齐。然后对每个机器人关节进行分段，并应用正向运动学变换，从而能够以任意关节配置渲染机器人。</p>
<h3 id="D-Object-Splat模型"><a href="#D-Object-Splat模型" class="headerlink" title="D. Object Splat模型"></a>D. Object Splat模型</h3><p>与机器人渲染类似，使用 ICP 来对齐每个对象的 3D 高斯 $\mathcal{S}^{k}_{obj}$​ 到其模拟的真实点云。</p>
<h3 id="F"><a href="#F" class="headerlink" title="F."></a>F.</h3>]]></content>
      <categories>
        <category>三维重建</category>
      </categories>
      <tags>
        <tag>3DGS</tag>
        <tag>Sim2Real</tag>
      </tags>
  </entry>
  <entry>
    <title>Mujoco - 高场hfield相关</title>
    <url>/2024/09/26/hfield/</url>
    <content><![CDATA[<blockquote>
<p>The <strong>hfield</strong> type defines a height field geom. The geom must reference the desired height field asset with the hfield attribute below. The position and orientation of the geom set the position and orientation of the height field. The size of the geom is ignored, and the size parameters of the height field asset are used instead. See the description of the <a class="link"   href="https://mujoco.readthedocs.io/en/stable/XMLreference.html#asset-hfield" >hfield <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> element. Similar to planes, height field geoms can only be attached to the world body or to static children of the world.</p>
</blockquote>
<ol>
<li><p>可从PNG的灰度图像加载高场数据。每个像素即为一个高度，黑低白高。</p>
</li>
<li><p>可从bin文件读入，格式如下：</p>
<div class="highlight-container" data-rel="Plaintext"><figure class="iseeu highlight plaintext"><table><tr><td class="code"><pre><span class="line">(int32)   nrow</span><br><span class="line">(int32)   ncol</span><br><span class="line">(float32) data[nrow*ncol]</span><br></pre></td></tr></table></figure></div>
</li>
<li><p>高度数据可以在编译时保持未定义。</p>
</li>
</ol>
<ul>
<li>编译器会自动把高度数据归一化到[0,1]</li>
<li>高场的位置和方向由geom确定，空间范围由hfield的size字段决定。（与mesh相同）</li>
<li>高场视为三棱柱的并集，碰撞时首先确认可能碰撞的棱柱网格，然后通过凸面碰撞器计算。高场和geom的碰撞上限限制为50，超过的则被舍弃。</li>
</ul>
<h2 id="参数"><a href="#参数" class="headerlink" title="参数"></a>参数</h2><ul>
<li><strong>name</strong>: 名称，用于引用。如果忽略name，可用不带路径和后缀名的文件名代替引用。</li>
<li><strong>content_type</strong>: 目前支持<code>image/png</code>和<code>image/vnd.mujoco.hfield</code>。</li>
<li><strong>file</strong>: 文件名，若后缀为<code>.png</code>（不区分大小写），则按图像读入；否则以二进制文件读入。</li>
<li><strong>nrow</strong>, <strong>ncol</strong>: 行数和列数。默认值 0 表示将从文件加载数据。</li>
<li><strong>elevation</strong>: 高场，自动归一，默认值0。</li>
<li><strong>size</strong>: (radius_x、radius_y、elevation_z、base_z)，分别是x、y方向的半径，最大高度，和基础厚度。</li>
</ul>
<h2 id="使用样例"><a href="#使用样例" class="headerlink" title="使用样例"></a>使用样例</h2><div class="highlight-container" data-rel="Xml"><figure class="iseeu highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">mujoco</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">asset</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">hfield</span> <span class="attr">file</span>=<span class="string">&quot;./data/height_field.bin&quot;</span> <span class="attr">name</span>=<span class="string">&quot;customTerrain&quot;</span> <span class="attr">ncol</span>=<span class="string">&quot;100&quot;</span> <span class="attr">nrow</span>=<span class="string">&quot;100&quot;</span> <span class="attr">size</span>=<span class="string">&quot;50 50 1 0.1&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">asset</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">worldbody</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">geom</span> <span class="attr">hfield</span>=<span class="string">&quot;customTerrain&quot;</span> <span class="attr">pos</span>=<span class="string">&quot;0 0 0&quot;</span> <span class="attr">type</span>=<span class="string">&quot;hfield&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">worldbody</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">mujoco</span>&gt;</span></span><br></pre></td></tr></table></figure></div>

]]></content>
      <categories>
        <category>机器仿真</category>
      </categories>
      <tags>
        <tag>mujoco</tag>
        <tag>hfield</tag>
      </tags>
  </entry>
  <entry>
    <title>Mujoco - 碰撞凸几何体要求</title>
    <url>/2024/09/29/mujoco_notes/</url>
    <content><![CDATA[<h2 id="Collision"><a href="#Collision" class="headerlink" title="Collision"></a>Collision</h2><p><a class="link"   href="https://mujoco.readthedocs.io/en/stable/computation/index.html#collision" >Computation - MuJoCo Documentation <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a></p>
<blockquote>
<p>We have chosen to limit collision detection to <em>convex</em> geoms. All primitive types are convex. Height fields are not convex but internally they are treated as unions of triangular prisms (using custom collision pruning beyond the filters described above). Meshes specified by the user can be non-convex, and are rendered as such. For collision purposes however they are replaced with their convex hulls.</p>
</blockquote>
<ul>
<li>碰撞检测限制在<em>凸</em>几何体</li>
<li>所有原始类型都是凸的</li>
<li>高度字段不是凸的，但在内部它们被视为三棱柱的并集</li>
<li>网格可以是非凸的，并且如此渲染。然而，出于碰撞目的，它们被替换为凸包</li>
</ul>
<blockquote>
<p>In order to model a non-convex object other than a height field, the user must decompose it into a union of convex geoms (which can be primitive shapes or meshes) and attach them to the same body. Open tools like the <a class="link"   href="https://github.com/SarahWeiii/CoACD" >CoACD library <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> can be used outside MuJoCo to automate this process. Finally, all built-in collision functions can be replaced with custom callbacks. This can be used to incorporate a general-purpose “triangle soup” collision detector for example. However we do not recommend such an approach. Pre-processing the geometry and representing it as a union of convex geoms takes some work, but it pays off at runtime and yields both faster and more stable simulation.</p>
<p>为了对高度场以外的非凸对象进行建模，用户必须将其分解为凸几何体（可以是原始形状或网格）的并集并将它们附加到同一实体。可以在 MuJoCo 外部使用<a class="link"   href="https://github.com/SarahWeiii/CoACD" >CoACD 库 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>等开放工具来自动化此过程。最后，所有内置碰撞函数都可以替换为自定义回调。例如，这可用于合并通用“三角汤”碰撞检测器。但是我们不推荐这种方法。预处理几何体并将其表示为凸几何体的联合需要一些工作，但它在运行时得到回报，并产生更快、更稳定的模拟。</p>
</blockquote>
]]></content>
      <categories>
        <category>机器仿真</category>
      </categories>
      <tags>
        <tag>mujoco</tag>
        <tag>convex</tag>
        <tag>collision</tag>
      </tags>
  </entry>
  <entry>
    <title>NeRF和3DGS对比</title>
    <url>/2024/10/30/NERFvs3DGS/</url>
    <content><![CDATA[<h2 id="NeRF"><a href="#NeRF" class="headerlink" title="NeRF"></a>NeRF</h2><h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>NeRF（Neural Radiance Fields）是一种新型的3D场景表示方法，通过神经网络来生成逼真的3D场景。NeRF 的核心思想是利用神经网络来表示一个场景的体素，从而可以实现从不同角度对场景进行渲染，生成高质量的图像。</p>
<p>特点：</p>
<ul>
<li>NeRF中三维模型的信息是以<strong>“隐式”</strong>的方法存储，而非点云、体素、网格等显式的表达方式。</li>
<li>NeRF使用类似<strong>光线追踪</strong>的方式创建新视角的图像。输入是采样点和观测的方向，输出是对应的RGB值和不透明度。</li>
</ul>
<h3 id="流程"><a href="#流程" class="headerlink" title="流程"></a>流程</h3><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/bcbfe7de8a71226465e395e9aca94b72.png"
                      alt="Refer"
                ></p>
<ol>
<li><strong>输入信息</strong>：NeRF 将3D空间中的点（例如摄像机的位置）和观察方向作为输入。</li>
<li><strong>神经网络映射</strong>：通过一个MLP（多层感知器）神经网络，将每个3D坐标点和观察方向映射到颜色和体素密度。网络的输出包含颜色（RGB）和体密度（衡量光线穿过该点的透明度）。</li>
<li><strong>体渲染技术</strong>：NeRF 使用体渲染公式，即光线穿过场景的过程。具体来说，它沿着光线方向对多个点进行采样，并根据体密度和颜色计算每个点的贡献，进而得出最终的像素值。</li>
<li><strong>损失函数</strong>：通过多视图监督进行训练，NeRF会在场景中从多个视角捕获图像，将渲染的像素值与真实图像进行对比，并通过优化损失函数不断调整网络参数，逐步逼近真实场景。</li>
<li><strong>生成新视角</strong>：训练完成后，NeRF 能够在未见过的视角上生成图像，实现自由的3D视角切换和逼真的场景合成。</li>
</ol>
<h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/340d8d0e46ba24f221761300ece496f8.png"
                      alt="network"
                ></p>
<ul>
<li><strong>不透明度</strong>只和空间位置有关，<strong>颜色</strong>与空间位置和视角有关。</li>
</ul>
<h3 id="实现细节"><a href="#实现细节" class="headerlink" title="实现细节"></a>实现细节</h3><h4 id="MLP网络"><a href="#MLP网络" class="headerlink" title="MLP网络"></a>MLP网络</h4><ul>
<li><strong>层数</strong>：NeRF的网络由10个隐藏层组成，前9层包含256个神经元，最后一层包含128个神经元。</li>
<li><strong>分支结构</strong>：NeRF的MLP有一个特定的分支结构。前8层用于处理空间位置信息(x,y,z)，生成了隐变量（latent features）。这些特征被用来预测体密度（density），以表示该点在场景中的不透明度。</li>
<li><strong>跳跃连接</strong>：位置编码后的输入会与网络中间层（第5层）相连，作为跳跃连接（skip connection），帮助网络在深层结构中保持空间信息的细节。</li>
</ul>
<h4 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h4><ul>
<li>NeRF 使用了傅里叶特征来将输入的3D坐标和视角方向进行编码，这种方式称为<strong>位置编码</strong>（Positional Encoding）。具体来说，它将输入扩展到高频率空间，这样网络就能够学习更细腻的细节。</li>
<li>对于坐标 (x,y,z)(x, y, z)(x,y,z) 和方向 (θ,ϕ)，每个输入维度会生成多个不同频率的正余弦特征，以捕捉场景中的复杂空间结构。</li>
</ul>
<h4 id="体渲染"><a href="#体渲染" class="headerlink" title="体渲染"></a>体渲染</h4><p>​	从焦点到一个像素上连的射线为<code>r(t)=o+td</code>，其中其中<code>o</code>是原点，<code>t</code>是距离。距离起点（near bound）和距离终点（far bound）为<code>tn</code>和<code>tf</code>。</p>
<p>​	获得像素颜色的公式：</p>
<p>​	<img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/66ccede850b01c372315e2b505c4ec7b.png"
                      alt="color"
                ></p>
<p>​	这个式子积分里面是<code>T(t)</code> 、<code>密度 σ(r(t))</code>和颜色<code>c(r(t),d)</code>的乘积，其中<code>T(t)</code>是累积透光率，表示光线射到这“还剩多少光”。实际渲染过程是把射线平均分成N个小区间，每个区间随机采样一个点，对采样得到的点的颜色进行某种加权求和。</p>
<h3 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h3><h4 id="NeRF2Mesh"><a href="#NeRF2Mesh" class="headerlink" title="NeRF2Mesh"></a>NeRF2Mesh</h4><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/9e1ed1cebbf7c2079a3bd129333c9730.png"
                      alt="NeRF2Mesh"
                ></p>
<p>最左侧的立方体就是Nerf所构建的三维数据，它包含离散点的三维坐标、不透明度（density）以及rgb色彩。<br>Nerf2Mesh的整体结构也像图中分为上下两个分支：</p>
<ul>
<li>**密度分支 (Density field)**：首先利用NeRF生成的密度场划分出体素，再用Marching Cubes生成三角网格，再进行优化。</li>
<li>**外观分支 (Appearance Field)**：Nerf输出的内容（RGB）经过MLP1提取特征，然后分成两个分支分别提取漫反射和镜面反射（镜面反射会多经过一个MLP层）的分量。</li>
</ul>
<h2 id="3D-Gaussian-Splatting"><a href="#3D-Gaussian-Splatting" class="headerlink" title="3D Gaussian Splatting"></a>3D Gaussian Splatting</h2><h3 id="原理-1"><a href="#原理-1" class="headerlink" title="原理"></a>原理</h3><p>3D Gaussian Splatting 的核心思想是用高斯分布来表示 3D 空间中的点云，将场景中的点用 3D 高斯函数表示。这些高斯函数即“高斯球”，通过一组参数（均值、协方差矩阵等）来描述位置、方向、大小和形状。相比于单纯的点云，使用高斯分布可以更好地对点的位置和形状进行逼真地表达，使得结果更平滑并且抗噪性更好。</p>
<p>在渲染过程中，场景以相机视角来观察这些高斯分布的点，生成一个图像。每个像素的颜色和透明度是通过聚合沿视线方向的高斯球信息来计算的。这些高斯分布产生的重叠区域，通过数学上的加权平均可以很好地呈现出自然的模糊边缘，避免了传统点云中颗粒状的视觉问题。</p>
<h3 id="流程-1"><a href="#流程-1" class="headerlink" title="流程"></a>流程</h3><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/b957eb22052d10a72d8d78c8416a0af4.png"
                      alt="3DGS"
                ></p>
<ol>
<li><strong>点云采样</strong>：使用SfM从一组图像中估计出点云，可以直接调用 <a class="link"   href="https://link.zhihu.com/?target=https://colmap.github.io/" >COLMAP <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a> 库操作</li>
<li><strong>高斯分布</strong>：将每个点分配一个高斯分布。初始高斯分布通过位置、颜色、透明度和协方差矩阵等参数来定义。</li>
<li><strong>投影变换</strong>：通过投影操作将 3D 高斯分布映射到相机视角下的 2D 图像平面。</li>
<li><strong>光栅化</strong>：对投影后的高斯分布进行渲染。</li>
<li><strong>优化</strong>：通过计算图像与目标图像之间的损失，反向传播梯度（Gradient Flow）来调整高斯参数。以及自适应地调整高斯分布的密度。</li>
</ol>
<h3 id="实现细节-1"><a href="#实现细节-1" class="headerlink" title="实现细节"></a>实现细节</h3><h4 id="三维高斯属性"><a href="#三维高斯属性" class="headerlink" title="三维高斯属性"></a>三维高斯属性</h4><p>用三维高斯分布构建基础元素，属性有中心<code>μ</code>、不透明度<code>α</code>、三维协方差矩阵（表示缩放程度）<code>Σ</code>和颜色<code>c</code>。其中颜色<code>c</code>与视角有关，由球谐函数表示。</p>
<p>球谐函数的示例：</p>
<p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/2b098bcc4d5038318f72a2e4fb51ef3b.jpeg"
                      alt="球谐函数"
                ></p>
<p>（图源：<a class="link"   href="https://zhuanlan.zhihu.com/p/679809915" >3DGS综述以及对3DGS的理解：A Survey on 3D Gaussian Splatting - 知乎 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>）</p>
<h4 id="自适应密度控制"><a href="#自适应密度控制" class="headerlink" title="自适应密度控制"></a>自适应密度控制</h4><p><img  
                     lazyload
                     src="/images/loading.svg"
                     data-src="https://pub-70fb49a5419e4021a1be66effc7bcf9e.r2.dev/2024/11/d8d3a1e3a41d17eeebed3b751bf58459.png"
                      alt="自适应密度控制"
                ></p>
<ul>
<li><strong>Under-Reconstruction</strong>: 复制一个当前高斯分布的副本，然后沿着位置梯度方向移动它。</li>
<li><strong>Over-Reconstruction</strong>: 将当前高斯分布分割成两个较小的高斯分布，再对其进行移动。</li>
</ul>
<h4 id="点的剪枝"><a href="#点的剪枝" class="headerlink" title="点的剪枝"></a>点的剪枝</h4><p>对于冗余的高斯分布，在迭代过程中会逐渐消除。不透明度太低的高斯分布和过大的高斯分布都会在迭代中逐渐消除。以节省资源。</p>
<h4 id="Tile"><a href="#Tile" class="headerlink" title="Tile"></a>Tile</h4><p>为了降低运算成本，3DGS将图像分割为数个不重叠的patch，称为<code>tile</code>，每个<code>tile</code>为<code>16×16</code>像素。在此基础上，3DGS计算投影后的高斯与<code>tile</code>的相交情况。由于高斯可能与多个<code>tile</code>相交，所以对其进行了复制，并为其分配<code>tile</code>的标识符。不同的<code>tile</code>可以在不同的线程或 GPU 核心上同时计算，避免了对同一像素的竞争。</p>
<h2 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h2><h3 id="场景绘制"><a href="#场景绘制" class="headerlink" title="场景绘制"></a>场景绘制</h3><ul>
<li>NeRF使用光线追踪来创建新视点的图像。通过模拟从像素位置发射的光线来计算图像中每个像素的颜色，运行速度较慢。但渲染更真实。</li>
<li>3DGS 使用光栅化来创建新视点的图像，通过Splatting进行渲染，运算速度更快，但3DGS创建了数百万个gaussion，占用的内存比NeRF多数倍。</li>
</ul>
<h3 id="3D重建"><a href="#3D重建" class="headerlink" title="3D重建"></a>3D重建</h3><ul>
<li>NeRF的输出需要借助其他方法转为显式输出，如NeRF2Mesh。</li>
<li>3DGS训练后最终得到的是一个文件，其中包含每个高斯的高斯参数列表，例如位置、大小、颜色和透明度。3DGS的输出是比较显式的表达。</li>
</ul>
<p>在CSDN上有一个3DGS和NeRF的对比表格，但数据来源并未标明，可靠性存疑（而且他的3DGS怎么是3D Geometry Sensing的缩写，虽然他给的结构图确实是3D高斯的）：</p>
<table>
<thead>
<tr>
<th><strong>对比维度</strong></th>
<th>3DGS (3D Geometry Sensing)</th>
<th>NeRF (Neural Radiance Fields)</th>
</tr>
</thead>
<tbody><tr>
<td><strong>基本原理</strong></td>
<td>基于几何推断，通过多视角图像、深度传感器、LiDAR等获取显式3D几何信息。</td>
<td>基于神经网络拟合体积辐射场，通过多视角图像学习隐式表示，渲染出场景。</td>
</tr>
<tr>
<td><strong>输入数据</strong></td>
<td>多视角图像、深度信息（LiDAR、ToF相机）、位姿数据、点云。</td>
<td>多视角图像（通常包括相机位姿），不需要显式的几何信息。</td>
</tr>
<tr>
<td><strong>输出结果</strong></td>
<td>点云、网格、三角形模型、深度图、纹理映射等显式几何结构。</td>
<td>通过体积渲染生成逼真图像（视角相关），不直接输出几何模型。</td>
</tr>
<tr>
<td><strong>数据处理方式</strong></td>
<td>使用几何关系（如三角测量、立体视觉等）来显式重建场景结构。</td>
<td>使用神经网络隐式建模颜色和密度，通过体积渲染生成图像。</td>
</tr>
<tr>
<td><strong>几何信息</strong></td>
<td>显式获取3D几何信息，可以精确测量物体的距离和形状。</td>
<td>隐式推断几何信息，主要用于图像渲染，几何结构不直接输出。</td>
</tr>
<tr>
<td><strong>渲染效果</strong></td>
<td>依赖于重建的几何结构，渲染效果有限，尤其在复杂光线场景下效果一般。</td>
<td>渲染效果非常逼真，尤其在反射、遮挡、折射等复杂光照场景表现优异。</td>
</tr>
<tr>
<td><strong>计算资源需求</strong></td>
<td>需要较强的几何计算能力，数据获取通常依赖于多传感器系统（LiDAR等）。</td>
<td>需要高计算资源，特别是训练神经网络的过程计算量大，通常依赖于GPU。</td>
</tr>
<tr>
<td><strong>渲染速度</strong></td>
<td>实时性较好，特别是有深度传感器时可实现快速重建。</td>
<td>渲染速度较慢，尤其在训练阶段耗时长，但有即时渲染版本。</td>
</tr>
<tr>
<td><strong>应用场景</strong></td>
<td>自动驾驶、机器人导航、工业检测、3D建模、AR&#x2F;VR、精密测量。</td>
<td>电影视觉特效、虚拟旅游、虚拟现实内容生成、复杂光照场景的渲染。</td>
</tr>
<tr>
<td><strong>优点</strong></td>
<td>可以显式建模、精确几何测量、适用于实时应用；深度传感器辅助时重建精度高。</td>
<td>渲染质量极高，能处理复杂的光照、遮挡问题；不需要昂贵的深度传感器。</td>
</tr>
<tr>
<td><strong>缺点</strong></td>
<td>在处理复杂光照（如透明物体、反射面）时效果不佳，依赖昂贵的传感器数据。</td>
<td>渲染速度慢，训练时间长，初始设计不擅长生成明确的几何信息。</td>
</tr>
<tr>
<td><strong>几何建模精度</strong></td>
<td>高，适合用于需要精确几何信息的场景（如测量、导航、物理模拟等）。</td>
<td>几何建模是隐式的，主要依赖于神经网络推断，不适合用于测量等任务。</td>
</tr>
<tr>
<td><strong>光照处理</strong></td>
<td>处理复杂光线条件较困难，通常需要额外的算法来应对光线反射和折射。</td>
<td>对复杂光照场景处理效果出色，能够处理反射、折射、遮挡等问题。</td>
</tr>
<tr>
<td><strong>实时性</strong></td>
<td>实时性较强，特别是在配合LiDAR等传感器时。</td>
<td>需要较长的时间进行训练和渲染，不适合实时应用（加速版本除外）。</td>
</tr>
<tr>
<td><strong>数据获取成本</strong></td>
<td>高，需要多视角相机或昂贵的深度传感器（如LiDAR）。</td>
<td>低，仅需多视角图像数据，不依赖于专门的传感器。</td>
</tr>
</tbody></table>
<p>（表源：<a class="link"   href="https://blog.csdn.net/Darlingqiang/article/details/142773142" >【3dgs】3DGS与NeRF对比_nerf和3dgs区别-CSDN博客 <i class="fa-regular fa-arrow-up-right-from-square fa-sm"></i></a>）</p>
]]></content>
      <categories>
        <category>三维重建</category>
      </categories>
      <tags>
        <tag>NeRF</tag>
        <tag>3DGS</tag>
      </tags>
  </entry>
</search>
